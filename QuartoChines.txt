Link :
https://plato.stanford.edu/entries/chinese-room/

O Argumento do Quarto Chinês
Publicado pela primeira vez em 19 de março de 2004; revisão substantiva quinta-feira, 20 de fevereiro de 2020
O argumento e experimento de pensamento agora geralmente conhecido como Argumento da Sala Chinesa foi publicado pela primeira vez em um artigo de 1980 pelo filósofo americano John Searle (1932–). Tornou-se um dos argumentos mais conhecidos da filosofia recente. Searle se imagina sozinho em uma sala seguindo um programa de computador para responder a caracteres chineses que deslizam por baixo da porta. Searle não entende nada de chinês e, no entanto, seguindo o programa de manipulação de símbolos e numerais da mesma forma que um computador, ele envia sequências apropriadas de caracteres chineses de volta para debaixo da porta, e isso leva os que estão fora a supor erroneamente que há um falante de chinês. no quarto.

A conclusão estreita do argumento é que programar um computador digital pode fazer com que ele pareça entender a linguagem, mas não pode produzir uma compreensão real. Portanto, o “Teste de Turing” é inadequado. Searle argumenta que o experimento mental ressalta o fato de que os computadores simplesmente usam regras sintáticas para manipular cadeias de caracteres de símbolos, mas não entendem o significado ou a semântica. A conclusão mais ampla do argumento é que a teoria de que as mentes humanas são computacionais ou sistemas de processamento de informações semelhantes a computadores é refutada. Em vez disso, as mentes devem resultar de processos biológicos; os computadores podem, na melhor das hipóteses, simular esses processos biológicos. Assim, o argumento tem grandes implicações para semântica, filosofia da linguagem e da mente, teorias da consciência, ciência da computação e ciência cognitiva em geral. Como resultado,

1. Visão Geral
2. Antecedentes Históricos
2.1 Moinho de Leibniz
2.2 Máquina de Papel de Turing
2.3 A Nação Chinesa
3. O Argumento da Sala Chinesa
4. Respostas ao Argumento da Sala Chinesa
4.1 A Resposta dos Sistemas
4.2 A Resposta do Robô
4.3 Resposta do Brain Simulator
4.4 A Resposta das Outras Mentes
4.5 A resposta da intuição
5. As Questões Filosóficas Maiores
5.1 Sintaxe e Semântica
5.2 Intencionalidade
5.3 Mente e Corpo
5.4 Simulação, duplicação e evolução
Conclusão
Bibliografia
Ferramentas acadêmicas
Outros recursos da Internet
Entradas Relacionadas
1. Visão Geral
O trabalho em Inteligência Artificial (IA) produziu programas de computador que podem vencer o campeão mundial de xadrez, controlar veículos autônomos, completar nossas frases de e-mail e derrotar os melhores jogadores humanos no programa de perguntas e respostas Jeopardy . A IA também produziu programas com os quais se pode conversar em linguagem natural, incluindo “agentes virtuais” de atendimento ao cliente, Alexa da Amazon e Siri da Apple . Nossa experiência mostra que jogar xadrez ou Jeopardye conversar, são atividades que exigem compreensão e inteligência. A proeza do computador em conversas e jogos desafiadores mostra que os computadores podem entender a linguagem e ser inteligentes? O desenvolvimento posterior resultará em computadores digitais que correspondam totalmente ou até excedam a inteligência humana? Alan Turing (1950), um dos teóricos pioneiros da computação, acreditava que a resposta a essas perguntas era “sim”. Turing propôs o que agora é conhecido como ' O Teste de Turing': se um computador pode se passar por humano em um bate-papo online, devemos admitir que ele é inteligente. No final da década de 1970, alguns pesquisadores de IA afirmaram que os computadores já entendiam pelo menos alguma linguagem natural. Em 1980, o filósofo da UC Berkeley, John Searle, introduziu um argumento curto e amplamente discutido com a intenção de mostrar conclusivamente que é impossível para os computadores digitais entender a linguagem ou pensar.

Searle argumenta que uma boa maneira de testar uma teoria da mente, digamos, uma teoria que sustenta que o entendimento pode ser criado fazendo isso ou aquilo, é imaginar como seria realmente fazer o que a teoria diz que criará o entendimento. Searle (1999) resumiu seu argumento do Quarto Chinês (doravante, CRA) concisamente:

Imagine um falante nativo de inglês que não sabe chinês trancado em uma sala cheia de caixas de símbolos chineses (uma base de dados) junto com um livro de instruções para manipular os símbolos (o programa). Imagine que as pessoas de fora da sala enviem outros símbolos chineses que, desconhecidos da pessoa na sala, são perguntas em chinês (a entrada). E imagine que, seguindo as instruções do programa, o homem na sala seja capaz de distribuir símbolos chineses que são as respostas corretas para as perguntas (a saída). O programa permite que a pessoa na sala passe no Teste de Turing para entender chinês, mas ela não entende uma palavra de chinês.
Searle continua dizendo: “O ponto do argumento é o seguinte: se o homem na sala não entende chinês com base na implementação do programa apropriado para entender chinês, nenhum outro computador digital o fará apenas com base nisso, porque nenhum computador , qua computador, tem tudo o que o homem não tem.”

Trinta anos após a introdução do CRA Searle 2010 descreve a conclusão em termos de consciência e intencionalidade :

Demonstrei anos atrás com o chamado Argumento da Sala Chinesa que a implementação do programa de computador não é por si só suficiente para a consciência ou intencionalidade (Searle 1980). A computação é definida puramente formal ou sintaticamente, enquanto as mentes têm conteúdos mentais ou semânticos reais, e não podemos passar do sintático ao semântico apenas tendo as operações sintáticas e nada mais. Para colocar este ponto um pouco mais tecnicamente, a noção “mesmo programa implementado” define uma classe de equivalência que é especificada independentemente de qualquer realização física específica. Mas tal especificação necessariamente deixa de fora os poderes biologicamente específicos do cérebro para causar processos cognitivos. Um sistema, eu, por exemplo,
“Intencionalidade” é um termo técnico para uma característica do mental e outras coisas, ou seja, ser sobre algo. Assim, um desejo por um pedaço de chocolate e pensamentos sobre a verdadeira Manhattan ou o fictício Harry Potter exibem intencionalidade, como será discutido com mais detalhes na seção 5.2 abaixo.

A mudança de Searle da compreensão da máquina para a consciência e a intencionalidade não é diretamente apoiada pelo argumento original de 1980. No entanto, a re-descrição da conclusão indica a estreita conexão entre compreensão e consciência nas explicações posteriores de significado e intencionalidade de Searle. Aqueles que não aceitam a conta de vinculação de Searle podem sustentar que a execução de um programa pode criar compreensão sem necessariamente criar consciência e, inversamente, um robô sofisticado pode ter consciência, desejos e crenças de nível canino, sem necessariamente entender a linguagem natural.

Ao passar para a discussão da intencionalidade, Searle procura desenvolver as implicações mais amplas de seu argumento. O objetivo é refutar a abordagem funcionalista para entender as mentes, ou seja, a abordagem que sustenta que os estados mentais são definidos por seus papéis causais, não pelas coisas (neurônios, transistores) que desempenham esses papéis. O argumento conta especialmente contra aquela forma de funcionalismo conhecida como Teoria Computacional da Mente. que trata as mentes como sistemas de processamento de informações. Como resultado de seu escopo, bem como do estilo de escrita claro e vigoroso de Searle, o argumento da Sala Chinesa provavelmente foi o argumento filosófico mais amplamente discutido na ciência cognitiva desde o Teste de Turing. Em 1991, o cientista da computação Pat Hayes definiu a Ciência Cognitiva como o projeto de pesquisa em andamento para refutar o argumento de Searle. O psicólogo cognitivo Steven Pinker (1997) apontou que, em meados da década de 1990, mais de 100 artigos foram publicados sobre o experimento mental de Searle – e essa discussão foi tão difundida na Internet que Pinker achou uma razão convincente para remover seu nome de todas as listas de discussão da Internet.

Esse interesse não diminuiu e o leque de conexões com o argumento se ampliou. Uma pesquisa no Google Scholar por “Searle Chinese Room” limitada ao período de 2010 a 2019 produziu mais de 2.000 resultados, incluindo artigos que fazem conexões entre o argumento e tópicos que vão desde a cognição incorporada ao teatro, passando pela psicoterapia até as visões pós-modernas da verdade e “nosso futuro pós-humano” – bem como discussões sobre grupos ou mentes coletivas e discussões sobre o papel das intuições na filosofia. Em 2007, uma empresa de jogos adotou o nome "The Chinese Room" em homenagem à "... crítica de Searle à IA - de que você poderia criar um sistema que desse a impressão de inteligência sem nenhuma inteligência interna real". Essa ampla gama de discussões e implicações é um tributo à clareza e centralidade simples do argumento.

2. Antecedentes Históricos
2.1 Moinho de Leibniz
O argumento de Searle tem quatro antecedentes importantes. O primeiro deles é um argumento apresentado pelo filósofo e matemático Gottfried Leibniz (1646-1716). Este argumento, muitas vezes conhecido como “Leibniz' Mill”, aparece na seção 17 da Monadologia de Leibniz . Como o argumento de Searle, o argumento de Leibniz assume a forma de um experimento mental. Leibniz nos pede para imaginar um sistema físico, uma máquina, que se comporta de tal maneira que supostamente pensa e tem experiências (“percepção”).

17. Além disso, deve-se confessar que a percepção e o que dela depende são inexplicáveis ​​por motivos mecânicos, isto é, por meio de figuras e movimentos. E supondo que houvesse uma máquina, construída de modo a pensar, sentir e ter percepção, poderia ser concebida como aumentada em tamanho, mantendo as mesmas proporções, de modo que alguém pudesse entrar nela como em um moinho. Sendo assim, deveríamos, ao examinar seu interior, encontrar apenas partes que funcionam umas sobre as outras, e nunca algo pelo qual explicar uma percepção. Assim, é numa substância simples, e não num composto ou numa máquina, que se deve procurar a percepção. [Tradução de Robert Latta]
Observe que a estratégia de Leibniz aqui é contrastar o comportamento aberto da máquina, que pode parecer produto do pensamento consciente, com o modo como a máquina opera internamente. Ele aponta que essas operações mecânicas internas são apenas partes que se movem de um ponto a outro, portanto não há nada que seja consciente ou que possa explicar o pensamento, o sentimento ou a percepção. Para Leibniz, os estados físicos não são suficientes nem constitutivos dos estados mentais.

2.2 Máquina de Papel de Turing
Um segundo antecedente do argumento da Sala Chinesa é a ideia de uma máquina de papel, um computador implementado por um ser humano. Essa ideia é encontrada no trabalho de Alan Turing, por exemplo, em “Intelligent Machinery” (1948). Turing escreve lá que escreveu um programa para uma “máquina de papel” para jogar xadrez. Uma máquina de papel é um tipo de programa, uma série de etapas simples como um programa de computador, mas escrito em linguagem natural (por exemplo, inglês) e implementado por um ser humano. O operador humano da máquina de jogar xadrez de papel não precisa (de outra forma) saber jogar xadrez. Tudo o que o operador faz é seguir as instruções para gerar jogadas no tabuleiro de xadrez. Na verdade, o operador nem precisa saber que está jogando xadrez – as strings de entrada e saída, como “N–QB7”, não precisam significar nada para o operador da máquina de papel.

Como parte do projeto da Segunda Guerra Mundial para decifrar a criptografia militar alemã, Turing havia escrito programas em inglês para “computadores” humanos, como esses trabalhadores especializados eram então conhecidos, e esses computadores humanos não precisavam saber o que os programas que implementavam estavam fazendo. .

Uma das razões pelas quais a ideia de uma máquina humana mais papel é importante é que ela já levanta questões sobre agência e compreensão semelhantes às do CRA. Suponha que eu esteja sozinho em uma sala fechada e siga um livro de instruções para manipular sequências de símbolos. Dessa forma, implemento uma máquina de papel que gera sequências de símbolos como “N-KB3” que escrevo em pedaços de papel e coloco por baixo da porta para alguém do lado de fora da sala. Suponha ainda que, antes de entrar na sala, eu não saiba jogar xadrez, ou mesmo que exista tal jogo. No entanto, sem que eu soubesse, na sala estou executando o programa de xadrez de Turing e as sequências de símbolos que gerei são notações de xadrez e são interpretadas como jogadas de xadrez por aqueles de fora da sala. Eles respondem deslizando os símbolos para seus próprios movimentos de volta por baixo da porta da sala. Se tudo o que você vê é a sequência resultante de movimentos exibidos em um tabuleiro de xadrez fora da sala, você pode pensar que alguém na sala sabe jogar xadrez muito bem. Agora eu sei jogar xadrez? Ou é o sistema (composto por mim, os manuais e o papel no qual manipulo sequências de símbolos) que está jogando xadrez? Se eu memorizar o programa e fizer as manipulações dos símbolos dentro da minha cabeça, saberei jogar xadrez, ainda que com uma fenomenologia estranha? Os estados conscientes de alguém importam se eles sabem ou não jogar xadrez? Se um computador digital implementa o mesmo programa, o computador então joga xadrez ou apenas simula isso? Agora eu sei jogar xadrez? Ou é o sistema (composto por mim, os manuais e o papel no qual manipulo sequências de símbolos) que está jogando xadrez? Se eu memorizar o programa e fizer as manipulações dos símbolos dentro da minha cabeça, saberei jogar xadrez, ainda que com uma fenomenologia estranha? Os estados conscientes de alguém importam se eles sabem ou não jogar xadrez? Se um computador digital implementa o mesmo programa, o computador então joga xadrez ou apenas simula isso? Agora eu sei jogar xadrez? Ou é o sistema (composto por mim, os manuais e o papel no qual manipulo sequências de símbolos) que está jogando xadrez? Se eu memorizar o programa e fizer as manipulações dos símbolos dentro da minha cabeça, saberei jogar xadrez, ainda que com uma fenomenologia estranha? Os estados conscientes de alguém importam se eles sabem ou não jogar xadrez? Se um computador digital implementa o mesmo programa, o computador então joga xadrez ou apenas simula isso? embora com uma fenomenologia estranha? Os estados conscientes de alguém importam se eles sabem ou não jogar xadrez? Se um computador digital implementa o mesmo programa, o computador então joga xadrez ou apenas simula isso? embora com uma fenomenologia estranha? Os estados conscientes de alguém importam se eles sabem ou não jogar xadrez? Se um computador digital implementa o mesmo programa, o computador então joga xadrez ou apenas simula isso?

Em meados do século, Turing estava otimista de que os próprios computadores eletrônicos recém-desenvolvidos logo seriam capazes de exibir um comportamento aparentemente inteligente, respondendo a perguntas feitas em inglês e conversando. Turing (1950) propôs o que hoje é conhecido como Teste de Turing: se um computador pudesse se passar por humano em um bate-papo on-line, deveria ser considerado inteligente.

Um terceiro antecedente do argumento de Searle foi o trabalho do colega de Searle em Berkeley, Hubert Dreyfus. Dreyfus foi um dos primeiros críticos das afirmações otimistas feitas pelos pesquisadores de IA. Em 1965, quando Dreyfus estava no MIT, ele publicou um relatório de cerca de cem páginas intitulado “Alchemy and Artificial Intelligence”. Dreyfus argumentou que as principais características da vida mental humana não podem ser capturadas por regras formais de manipulação de símbolos. Dreyfus mudou-se para Berkeley em 1968 e em 1972 publicou sua extensa crítica, “What Computers Can't Do”. Os principais interesses de pesquisa de Dreyfus estavam na filosofia continental, com foco na consciência, intencionalidade e no papel da intuição e do pano de fundo inarticulado na formação de nossos entendimentos. Dreyfus identificou várias suposições problemáticas na IA, incluindo a visão de que os cérebros são como computadores digitais e,

No entanto, no final da década de 1970, à medida que os computadores se tornaram mais rápidos e baratos, alguns membros da crescente comunidade de IA começaram a alegar que seus programas podiam entender frases em inglês, usando um banco de dados de informações básicas. O trabalho de um deles, o pesquisador de Yale Roger Schank (Schank & Abelson 1977) chamou a atenção de Searle. Schank desenvolveu uma técnica chamada “representação conceitual” que usava “scripts” para representar relações conceituais (relacionadas à Semântica de Papel Conceitual). O argumento de Searle foi originalmente apresentado como uma resposta à alegação de que programas de IA, como o de Schank, literalmente entendem as sentenças às quais respondem.

2.3 A Nação Chinesa
Um quarto antecedente ao argumento da Sala Chinesa são os experimentos mentais envolvendo uma miríade de seres humanos agindo como um computador. Em 1961, Anatoly Mickevich (pseudônimo A. Dneprov) publicou “The Game”, uma história na qual um estádio cheio de 1.400 estudantes de matemática é organizado para funcionar como um computador digital (ver Dneprov 1961 e a tradução em inglês listada em Mickevich 1961, Other Internet Recursos). Por 4 horas, cada um faz repetidamente um pouco de cálculo em números binários recebidos de alguém próximo a eles e, em seguida, passa o resultado binário para alguém próximo. Eles descobrem no dia seguinte que traduziram coletivamente uma frase do português para o russo nativo. O protagonista de Mickevich conclui: “Provamos que mesmo a simulação mais perfeita do pensamento da máquina não é o processo de pensamento em si, que é uma forma superior de movimento da matéria viva. ”Aparentemente de forma independente, uma consideração semelhante surgiu na discussão inicial das teorias funcionalistas da mente e da cognição (ver discussão adicional na seção 5.3 abaixo). Os funcionalistas sustentam que os estados mentais são definidos pelo papel causal que desempenham em um sistema é definido pelo que faz, não pelo que é feito). Os críticos do funcionalismo foram rápidos em virar sua virtude proclamada de realizabilidade múltipla contra ele. Embora o funcionalismo fosse consistente com uma compreensão materialista ou biológica dos estados mentais (possivelmente uma virtude), ele não identificava tipos de estados mentais (como sentir dor ou se perguntar sobre OZ) com tipos particulares de estados neurofisiológicos, como “tipo-tipo teoria da identidade” fez. Em contraste com a teoria da identidade tipo-tipo, o funcionalismo permitiu que seres sencientes com fisiologia diferente tivessem os mesmos tipos de estados mentais que os humanos – dores, por exemplo. Mas foi apontado que se alienígenas extraterrestres, com algum outro sistema complexo no lugar de cérebros, pudessem realizar as propriedades funcionais que constituíam estados mentais, então, presumivelmente, sistemas ainda menos parecidos com cérebros humanos poderiam. A forma computacional do funcionalismo, que sustenta que o papel definidor de cada estado mental é seu papel no processamento ou computação da informação, é particularmente vulnerável a essa manobra, uma vez que uma grande variedade de sistemas com componentes simples são computacionalmente equivalentes (ver, por exemplo, Maudlin 1989 para discussão de um computador construído com baldes de água). Os críticos perguntaram se era realmente plausível que esses sistemas inorgânicos pudessem ter estados mentais ou sentir dor.

Daniel Dennett (1978) relata que em 1974 Lawrence Davis deu um colóquio no MIT no qual apresentou uma dessas implementações pouco ortodoxas. Dennett resume o experimento mental de Davis da seguinte forma:

Que uma teoria funcionalista da dor (quaisquer que sejam seus detalhes) seja instanciada por um sistema cujos subconjuntos não sejam coisas como fibras C e sistemas reticulares, mas linhas telefônicas e escritórios administrados por pessoas. Talvez seja um robô gigante controlado por um exército de seres humanos que o habitam. Quando as condições funcionalmente caracterizadas pela teoria para a dor forem satisfeitas, devemos dizer, se a teoria for verdadeira, que o robô está com dor. Ou seja, a dor real, tão real quanto a nossa, existiria em virtude da atuação talvez desinteressada e mercantilista dessas equipes burocráticas, executando suas próprias funções.
Em “Troubles with Functionalism”, também publicado em 1978, Ned Block visualiza toda a população da China implementando as funções dos neurônios no cérebro. Este cenário foi posteriormente chamado de “A Nação Chinesa” ou “O Ginásio Chinês”. Podemos supor que cada cidadão chinês receberia uma lista de chamadas de números de telefone e, em um horário predefinido no dia da implementação, os cidadãos “input” designados iniciariam o processo ligando para aqueles em sua lista de chamadas. Quando o telefone de qualquer cidadão tocava, ele ou ela telefonaria para aqueles em sua lista, que por sua vez entrariam em contato com outros. Nenhuma mensagem telefônica precisa ser trocada; tudo o que é necessário é o padrão de chamada. As listas de chamadas seriam construídas de forma que os padrões de chamadas implementassem os mesmos padrões de ativação que ocorrem entre neurônios no cérebro de alguém quando essa pessoa está em um estado mental – dor, por exemplo. As ligações telefônicas desempenham o mesmo papel funcional que os neurônios que provocam o disparo mútuo. Block estava interessado principalmente em qualia e, em particular, se é plausível sustentar que a população da China pode sofrer coletivamente, enquanto nenhum membro individual da população experimentou qualquer dor, mas o experimento mental se aplica a quaisquer estados e operações mentais , incluindo a compreensão da linguagem.

Assim, o experimento mental precursor de Block, assim como os de Davis e Dennett, é um sistema de muitos humanos em vez de um. O foco está na consciência, mas na medida em que o argumento de Searle também envolve a consciência, o experimento mental está intimamente relacionado ao de Searle. Cole (1984) tenta bombear intuições na direção inversa, estabelecendo um experimento de pensamento em que cada um de seus neurônios é consciente e totalmente consciente de suas ações, incluindo ser encharcado de neurotransmissores, sofrer potenciais de ação e esguichar neurotransmissores em seus vizinhos . Cole argumenta que seus neurônios conscientes achariam implausível que sua atividade coletiva produzisse uma consciência e outras competências cognitivas, incluindo a compreensão do inglês, que faltam aos neurônios.

3. O Argumento da Sala Chinesa
Em 1980, John Searle publicou “Minds, Brains and Programs” na revista The Behavioral and Brain Sciences . Neste artigo, Searle apresenta o argumento e, em seguida, responde à meia dúzia de objeções principais que foram levantadas durante suas apresentações anteriores em vários campi universitários (ver a próxima seção). Além disso, o artigo de Searle no BBS foi publicado junto com comentários e críticas de 27 pesquisadores da ciência cognitiva. Esses 27 comentários foram seguidos pelas respostas de Searle aos seus críticos.

Nas décadas seguintes à sua publicação, o argumento da Sala Chinesa foi objeto de muitas discussões. Em 1984, Searle apresentou o argumento da Sala Chinesa em um livro, Minds, Brains and Science. Em janeiro de 1990, o popular periódico Scientific American levou o debate a um público científico geral. Searle incluiu o argumento da sala chinesa em sua contribuição, “Is the Brain's Mind a Computer Program?”, e o artigo de Searle foi seguido por um artigo de resposta, “Could a Machine Think?”, escrito pelos filósofos Paul e Patricia Churchland. Logo depois, Searle teve uma troca publicada sobre a Sala Chinesa com outro importante filósofo, Jerry Fodor (em Rosenthal (ed.) 1991).

O cerne do argumento é Searle imaginando-se seguindo um programa de processamento de símbolos escrito em inglês (que é o que Turing chamou de “uma máquina de papel”). O falante de inglês (Searle) sentado na sala segue instruções em inglês para manipular símbolos chineses, enquanto um computador “segue” (em certo sentido) um programa escrito em uma linguagem de computação. O humano produz a aparência de entender o chinês seguindo as instruções de manipulação do símbolo, mas não chega a entender o chinês. Uma vez que um computador apenas faz o que o ser humano faz – manipular símbolos com base apenas em sua sintaxe – nenhum computador, simplesmente seguindo um programa, chega a compreender genuinamente o chinês.

Esse argumento estreito, baseado no cenário da Sala Chinesa, é especificamente direcionado a uma posição que Searle chama de “IA Forte”. IA forte é a visão de que computadores adequadamente programados (ou os próprios programas) podem entender a linguagem natural e realmente ter outras capacidades mentais semelhantes aos humanos cujo comportamento eles imitam. De acordo com a Strong AI, esses computadores realmente jogam xadrez de forma inteligente, fazem jogadas inteligentes ou entendem a linguagem. Por outro lado, “IA fraca” é a afirmação muito mais modesta de que os computadores são meramente úteis em psicologia, linguística e outras áreas, em parte porque podem simular habilidades mentais. Mas a IA fraca não afirma que os computadores realmente entendem ou são inteligentes. O argumento da Sala Chinesa não é direcionado à IA fraca, nem pretende mostrar que nenhuma máquina pode pensar – Searle diz que cérebros são máquinas, e cérebros pensam. O argumento é direcionado à visão de que computações formais em símbolos podem produzir pensamento.

Podemos resumir o argumento estreito como um reductio ad absurdum contra a IA forte da seguinte forma. Seja L uma linguagem natural e digamos que um “programa para L” é um programa para conversar fluentemente em L. Um sistema de computação é qualquer sistema, humano ou não, que pode executar um programa.

Se a IA forte for verdadeira, então existe um programa para chinês de modo que, se algum sistema de computação executar esse programa, esse sistema passará a entender o chinês.
Eu poderia executar um programa para chinês sem com isso chegar a entender chinês.
Portanto, a IA forte é falsa.
A primeira premissa elucida a alegação de IA Forte. A segunda premissa é apoiada pelo experimento mental da Sala Chinesa. A conclusão desse argumento limitado é que a execução de um programa não pode dotar o sistema de compreensão da linguagem. (Existem outras maneiras de entender a estrutura do argumento. Pode ser relevante entender algumas das afirmações como contrafactuais: por exemplo, “existe um programa” na premissa 1 como significando que poderia haver um programa etc. argumento envolve lógica modal, a lógica da possibilidade e necessidade (ver Damper 2006 e Shaffer 2009)).

Vale notar também que a primeira premissa acima atribui o entendimento ao “sistema”. Exatamente o que o Strong-AI supõe que adquirirá entendimento quando o programa for executado é crucial para o sucesso ou fracasso do CRA. Schank 1978 tem um título que afirma que o computador de seu grupo, um dispositivo físico, entende, mas no corpo do artigo ele afirma que o programa [“SAM”] está fazendo o entendimento: SAM, Schank diz “...entende histórias sobre domínios sobre os quais tem conhecimento” (p. 133). Como veremos na próxima seção (4), essas questões sobre a identidade do entendedor (a CPU? O programa? O sistema? Outra coisa?) rapidamente vieram à tona para os críticos do CRA. O argumento mais amplo de Searle inclui a alegação de que o experimento mental mostra de forma mais geral que não se pode obter semântica (significado) da sintaxe (manipulação formal de símbolos). Isso e questões relacionadas são discutidas na seção 5: As Questões Filosóficas Maiores.

4. Respostas ao Argumento da Sala Chinesa
As críticas ao estreito argumento da Sala Chinesa contra a IA forte geralmente seguem três linhas principais, que podem ser distinguidas pelo quanto concedem:

(1) Alguns críticos admitem que o homem na sala não entende chinês, mas sustentam que, mesmo assim, a execução do programa pode criar a compreensão do chinês por alguém que não seja o operador da sala. Esses críticos se opõem à inferência da alegação de que o homem na sala não entende chinês para a conclusão de que nenhum entendimentofoi criado. Pode haver compreensão por uma entidade maior, menor ou diferente. Essa é a estratégia da The Systems Reply e da Virtual Mind Reply. Essas respostas sustentam que a saída da sala pode refletir a compreensão real do chinês, mas a compreensão não seria a do operador da sala. Assim, a alegação de Searle de que ele não entende chinês enquanto administra a sala é aceita, mas sua alegação de que não há compreensão das perguntas em chinês e que o computacionalismo é falso é negada.

(2) Outros críticos admitem a alegação de Searle de que apenas executar um programa de processamento de linguagem natural, conforme descrito no cenário CR, não cria nenhum entendimento, seja por um humano ou por um sistema de computador. Mas esses críticos sustentam que uma variação no sistema de computador poderia entender. A variante pode ser um computador embutido em um corpo robótico, interagindo com o mundo físico por meio de sensores e motores (“The Robot Reply”), ou pode ser um sistema que simula a operação detalhada de um cérebro humano inteiro, neurônio por neurônio. (“a resposta do Brain Simulator”).

(3) Finalmente, alguns críticos não concedem nem mesmo o ponto estreito contra a IA. Esses críticos sustentam que o homem no cenário original da Sala Chinesa pode entender chinês, apesar das negações de Searle, ou que o cenário é impossível. Por exemplo, os críticos argumentaram que nossas intuições nesses casos não são confiáveis. Outros críticos sustentaram que tudo depende do que se entende por “compreender” – pontos discutidos na seção sobre a resposta da intuição. Outros (por exemplo, Sprevak 2007) se opõem à suposição de que qualquer sistema (por exemplo, Searle na sala) pode executar qualquer programa de computador. E, finalmente, alguns argumentaram que, se não é razoável atribuir compreensão com base no comportamento exibido pela Sala Chinesa, então não seria razoável atribuir a compreensão aos humanos com base em evidências comportamentais semelhantes (Searle chama esta última de “resposta de outras mentes”). A objeção é que deveríamos estar dispostos a atribuir entendimento na Sala Chinesa com base no comportamento aberto, assim como fazemos com outros humanos (e alguns animais) e como faríamos com alienígenas extraterrestres (ou arbustos ardentes). ou anjos) que falavam nossa língua. Essa posição é próxima da do próprio Turing, quando ele propôs seu teste comportamental para a inteligência das máquinas.

Além dessas respostas especificamente ao cenário da Sala Chinesa e do argumento estreito a ser discutido aqui, alguns críticos também argumentam independentemente contra a afirmação mais ampla de Searle e sustentam que se pode obter semântica (isto é, significado) da manipulação de símbolos sintáticos, incluindo a tipo que ocorre dentro de um computador digital, questão discutida na seção abaixo sobre Sintaxe e Semântica.

4.1 A Resposta dos Sistemas
No artigo original do BBS, Searle identificou e discutiu várias respostas ao argumento que ele encontrou ao apresentar o argumento em palestras em vários lugares. Como resultado, essas primeiras respostas receberam mais atenção nas discussões subsequentes. O que Searle 1980 chama de “talvez a resposta mais comum” é a Resposta do Sistema.

A Systems Reply (que Searle diz ter sido originalmente associada a Yale, a casa do trabalho de IA de Schank) admite que o homem na sala não entende chinês. Mas, continua a resposta, o homem é apenas uma parte, uma unidade central de processamento (CPU), em um sistema maior. O sistema maior inclui o enorme banco de dados, a memória (scratchpads) contendo estados intermediários e as instruções – o sistema completo necessário para responder às perguntas chinesas. Portanto, a resposta do sistema é que, embora o homem que executa o programa não entenda chinês, o sistema como um todo entende.

Ned Block foi um dos primeiros a pressionar o Systems Reply, junto com muitos outros, incluindo Jack Copeland, Daniel Dennett, Douglas Hofstadter, Jerry Fodor, John Haugeland, Ray Kurzweil e Georges Rey. Rey (1986) diz que a pessoa na sala é apenas a CPU do sistema. Kurzweil (2002) diz que o ser humano é apenas um implementador e sem importância (presumivelmente significando que as propriedades do implementador não são necessariamente as do sistema). Kurzweil segue o espírito do Teste de Turing e sustenta que se o sistema exibe a aparente capacidade de entender chinês “teria que, de fato, entender chinês” – Searle está se contradizendo ao dizer, de fato, “a máquina fala chinês, mas não não entendo chinês”.

Margaret Boden (1988) levanta considerações sobre níveis. “A psicologia computacional não atribui ao cérebro a visão de brotos de feijão ou a compreensão do inglês: estados intencionais como esses são propriedades de pessoas, não de cérebros” (244). “Em suma, a descrição de Searle do pseudo-cérebro do robô (isto é, de Searle-in-the-robot) como entendendo o inglês envolve um erro de categoria comparável a tratar o cérebro como o portador, em oposição à base causal, de inteligência". Boden (1988) aponta que o operador da sala é um agente consciente, enquanto a CPU em um computador não é – o cenário da Sala Chinesa nos pede para ter a perspectiva do implementador, e não surpreendentemente falha em ver o quadro maior.

A resposta de Searle ao Systems Reply é simples: em princípio, ele poderia internalizar todo o sistema, memorizando todas as instruções e o banco de dados e fazendo todos os cálculos de cabeça. Ele poderia então sair da sala e passear ao ar livre, talvez até conversando em chinês. Mas ele ainda não teria como atribuir “qualquer significado aos símbolos formais”. O homem agora seria todo o sistema, mas ainda não entenderia chinês. Por exemplo, ele não saberia o significado da palavra chinesa para hambúrguer. Ele ainda não consegue obter a semântica da sintaxe.

De certa forma, a resposta de Searle aqui antecipa visões mentais estendidas posteriores (por exemplo, Clark e Chalmers 1998): se Otto, que sofre perda de memória, pode recuperar essas habilidades de recordação externando algumas das informações em seus cadernos, então Searle pode fazer o contrário : ao internalizar as instruções e cadernos, ele deveria adquirir todas as habilidades do sistema estendido. E assim Searle efetivamente conclui que, uma vez que ele não adquire compreensão do chinês internalizando os componentes externos de todo o sistema (por exemplo, ele ainda não sabe o que significa a palavra chinesa para hambúrguer), a compreensão nunca existiu no parcialmente exteriorizado sistema da Sala Chinesa original.

Em seu artigo de 2002 “The Chinese Room from a Logical Point of View”, Jack Copeland considera a resposta de Searle à Resposta dos Sistemas e argumenta que um homúnculo dentro da cabeça de Searle pode entender, embora o próprio operador da sala não entenda, assim como os módulos nas mentes resolvem equações de tensores que nos permitem pegar bolas de críquete. Copeland então se volta para considerar o Ginásio Chinês e novamente parece endossar a Resposta do Sistema: “…os jogadores individuais [não] entendem chinês. Mas não há nenhuma implicação disso para a alegação de que a simulação como um todo não compreende o chinês. A falácia envolvida em passar da parte para o todo é ainda mais gritante aqui do que na versão original do Argumento da Sala Chinesa”. Copeland nega que o conexionismo implique que uma sala cheia de pessoas possa simular o cérebro.

John Haugeland escreve (2002) que a resposta de Searle à Resposta dos Sistemas é falha: “… o que ele agora pergunta é como seria se ele, em sua própria mente, implementasse conscientemente as estruturas e operações formais subjacentes que a teoria diz são suficientes para implementar outra mente”. De acordo com Haugeland, sua incapacidade de entender o chinês é irrelevante: ele é apenas o implementador. O sistema maior implementado entenderia – há uma falácia de nível de descrição.

Shaffer 2009 examina os aspectos modais da lógica do CRA e argumenta que as versões familiares do System Reply são uma petição de princípio. Mas, afirma Shaffer, uma versão modalizada do System Reply é bem-sucedida porque existem mundos possíveis nos quais a compreensão é uma propriedade emergente da manipulação de sintaxe complexa. Nute 2011 é uma resposta a Shaffer.

Stevan Harnad defendeu o argumento de Searle contra os críticos da Systems Reply em dois artigos. Em seu artigo de 1989, Harnad escreve: “Searle formula o problema da seguinte forma: a mente é um programa de computador? Ou, mais especificamente, se um programa de computador simula ou imita nossas atividades que parecem exigir compreensão (como a comunicação em linguagem), pode-se dizer que o próprio programa entende ao fazê-lo?” (Observe a afirmação específica: a questão é se o próprio programa entende.) Harnad conclui: “Em face disso, [o argumento CR] parece válido. Certamente funciona contra a réplica mais comum, a 'Resposta do Sistema'….” Harnad parece seguir Searle ao vincular compreensão e estados de consciência: Harnad 2012 (Outros recursos da Internet) argumenta que Searle mostra que o problema central do “sentimento” consciente requer conexões sensoriais com o mundo real. (Veja as seções abaixo “A resposta do robô” e “Intencionalidade” para discussão.)

Finalmente, alguns argumentaram que mesmo que o operador de sala memorize as regras e execute todas as operações dentro de sua cabeça, o operador de sala não se torna o sistema. Cole (1984) e Block (1998) argumentam que o resultado não seria a identidade de Searle com o sistema, mas muito mais como um caso de personalidade múltipla – pessoas distintas em uma única cabeça. O sistema de resposta chinês não seria Searle, mas uma subparte dele. No caso CR, uma pessoa (Searle) é um monoglota inglês e o outro é um monoglota chinês. O total desconhecimento da pessoa que fala inglês sobre o significado das respostas chinesas não mostra que elas não são compreendidas. Esta linha, de pessoas distintas, leva à Virtual Mind Reply.

4.1.1 A Resposta da Mente Virtual
A resposta da Virtual Mind admite, assim como a System Reply, que o operador da Sala Chinesa não entende chinês meramente operando a máquina de papel. No entanto, a resposta da Virtual Mind sustenta que o importante é se a compreensão é criada, não se o operador da Sala é o agente que entende. Ao contrário da resposta do sistema, a resposta da mente virtual (VMR) sustenta que um sistema em execução pode criar novas entidades virtuais que são distintas do sistema como um todo, bem como dos subsistemas, como a CPU ou o operador. Em particular, um sistema em execução pode criar um agente distinto que entenda chinês. Esse agente virtual seria distinto tanto do operador da sala quanto de todo o sistema. Os traços psicológicos, incluindo habilidades linguísticas, de qualquer mente criada por inteligência artificial dependerá inteiramente do programa e do banco de dados chinês, e não será idêntico aos traços psicológicos e habilidades de uma CPU ou do operador de uma máquina de papel, como Searle no cenário da Sala Chinesa. De acordo com o VMR, o erro no argumento da sala chinesa é fazer a afirmação de que a IA forte é “o computador entende chinês” ou “o sistema entende chinês”. A alegação em questão para a IA deveria ser simplesmente se “o computador em execução cria compreensão do chinês”. De acordo com o VMR, o erro no argumento da sala chinesa é fazer a afirmação de que a IA forte é “o computador entende chinês” ou “o sistema entende chinês”. A alegação em questão para a IA deveria ser simplesmente se “o computador em execução cria compreensão do chinês”. De acordo com o VMR, o erro no argumento da sala chinesa é fazer a afirmação de que a IA forte é “o computador entende chinês” ou “o sistema entende chinês”. A alegação em questão para a IA deveria ser simplesmente se “o computador em execução cria compreensão do chinês”.

Um modelo familiar de agentes virtuais são personagens em jogos de computador ou videogame e assistentes pessoais digitais, como a Siri da Apple e a Cortana da Microsoft. Esses personagens têm várias habilidades e personalidades, e os personagens não são idênticos ao hardware do sistema ou programa que os cria. Um único sistema em execução pode controlar dois agentes distintos, ou robôs físicos, simultaneamente, um dos quais conversa apenas em chinês e um dos quais pode conversar apenas em inglês e que, de outra forma, manifestam personalidades, memórias e habilidades cognitivas muito diferentes. Assim, a resposta do VM nos pede para distinguir entre mentes e seus sistemas de realização.

Minsky (1980) e Sloman e Croucher (1980) sugeriram uma resposta da Mente Virtual quando o argumento da Sala Chinesa apareceu pela primeira vez. Em seu amplamente lido artigo de 1989 “Computação e Consciência”, Tim Maudlin considera sistemas físicos mínimos que podem implementar um sistema computacional executando um programa. Sua discussão gira em torno de sua máquina Olympia imaginária, um sistema de baldes que transfere água, implementando uma máquina de Turing. O alvo principal de Maudlin é a alegação dos computacionalistas de que tal máquina poderia ter consciência fenomenal. No entanto, no decorrer de sua discussão, Maudlin considera o argumento da Sala Chinesa. Maudlin (citando Minsky e Sloman e Croucher) aponta uma resposta da Virtual Mind de que o agente que entende pode ser distinto do sistema físico (414).

Perlis (1992), Chalmers (1996) e Block (2002) aparentemente também endossaram versões de uma resposta da Virtual Mind, assim como Richard Hanley em The Metaphysics of Star Trek(1997). Penrose (2002) é um crítico dessa estratégia, e Stevan Harnad rejeita desdenhosamente tais recursos heróicos à metafísica. Harnad defendeu a posição de Searle em um “Simpósio Virtual sobre Mentes Virtuais” (1992) contra Patrick Hayes e Don Perlis. Perlis pressionou um argumento de mentes virtuais derivado, diz ele, de Maudlin. Chalmers (1996) observa que o operador de sala é apenas um facilitador causal, um “demônio”, de modo que seus estados de consciência são irrelevantes para as propriedades do sistema como um todo. Como Maudlin, Chalmers levanta questões de identidade pessoal – podemos considerar a Sala Chinesa como “dois sistemas mentais realizados dentro do mesmo espaço físico. A organização que dá origem às experiências chinesas é bastante distinta da organização que dá origem às experiências do demónio [= operador de quarto]”(326).

Cole (1991, 1994) desenvolve a resposta e argumenta da seguinte forma: O argumento de Searle exige que o agente do entendimento seja o próprio computador ou, no paralelo da Sala Chinesa, a pessoa na sala. No entanto, o fracasso de Searle em entender o chinês na sala não mostra que não há entendimento sendo criado. Uma das principais considerações é que, na discussão de Searle, a conversa real com a Sala Chinesa é sempre seriamente subespecificada. Searle estava considerando os programas de Schank, que respondem apenas a algumas perguntas sobre o que aconteceu em um restaurante, tudo em terceira pessoa. Mas Searle deseja que suas conclusões se apliquem a quaisquer respostas produzidas por IA, incluindo aquelas que passariam no teste de Turing irrestrito mais difícil, ou seja, seriam exatamente o tipo de conversa que as pessoas reais têm umas com as outras. respostas de Searle . Searle não é o autor das respostas, e suas crenças e desejos, memórias e traços de personalidade (além de sua diligência!) Isso sugere que a seguinte condicional é verdadeira: se houver entendimento do chinês criado pela execução do programa, a mente que entende o chinês não seria o computador, seja o computador humano ou eletrônico. A pessoa que entendesse o chinês seria uma pessoa distinta da operadora de sala, com crenças e desejos conferidos pelo programa e seu banco de dados. Portanto, o fracasso de Searle em entender o chinês enquanto operava a sala não mostra que a compreensão não está sendo criada.

Cole (1991) oferece um argumento adicional de que a mente que faz a compreensão não é nem a mente do operador da sala nem o sistema que consiste no operador e no programa: executar um programa de computador adequadamente estruturado pode produzir respostas enviadas em chinês e também respostas a perguntas enviado em coreano. No entanto, as respostas chinesas podem aparentemente exibir conhecimentos e memórias, crenças e desejos completamente diferentes das respostas às perguntas coreanas – junto com a negação de que o respondente chinês conheça qualquer coreano e vice-versa. Assim, a evidência comportamental seria que havia duas mentes não idênticas (uma que entendia apenas chinês e outra que entendia apenas coreano). Uma vez que estes podem ter propriedades mutuamente exclusivas, eles não podem ser idênticos e, ipso facto, não pode ser idêntico à mente do implementador na sala. Analogamente, um videogame pode incluir um personagem com um conjunto de habilidades cognitivas (inteligente, entende chinês), bem como outro personagem com um conjunto incompatível (estúpido, monoglota inglês). Esses traços cognitivos inconsistentes não podem ser traços do sistema XBOX que os realiza. Cole argumenta que a implicação é que as mentes geralmente são mais abstratas do que os sistemas que as realizam (ver Mind and Body na seção Larger Philosophical Issues).

Em suma, o argumento da mente virtual é que, uma vez que a evidência que Searle fornece de que não há entendimento de chinês é que ele não entenderia chinês na sala, o argumento da sala chinesa não pode refutar uma afirmação de IA igualmente forte formulada de maneira diferente, afirmando a possibilidade de criar compreensão usando um computador digital programado. Maudlin (1989) diz que Searle não respondeu adequadamente a essa crítica.

Outros, no entanto, responderam ao VMR, incluindo Stevan Harnad e o físico matemático Roger Penrose. Penrose geralmente simpatiza com os pontos levantados por Searle com o argumento da Sala Chinesa e argumentou contra a resposta da Virtual Mind. Penrose não acredita que os processos computacionais possam explicar a consciência, tanto por motivos da Sala Chinesa, quanto por causa das limitações dos sistemas formais reveladas pela prova de incompletude de Kurt Gödel. (Penrose tem dois livros sobre mente e consciência; Chalmers e outros responderam aos apelos de Penrose a Gödel). variação – com uma sala ampliada para o tamanho da Índia, com índios fazendo o processamento – mostra que é muito implausível sustentar que há “algum tipo de 'entendimento' desencarnado associado à realização desse algoritmo pela pessoa e cuja presença não interfere de forma alguma em sua própria consciência” (230- 1). Penrose conclui que o argumento da Sala Chinesa refuta a IA Forte. Christian Kaernbach (2005) relata que submeteu a teoria da mente virtual a um teste empírico, com resultados negativos.

4.2 A Resposta do Robô
The Robot Reply admite que Searle está certo sobre o cenário da Sala Chinesa: mostra que um computador preso em uma sala de informática não consegue entender a linguagem ou saber o que as palavras significam. A resposta do Robô responde ao problema de saber o significado da palavra chinesa para hambúrguer – o exemplo de Searle de algo que o operador da sala não saberia. Parece razoável sustentar que a maioria de nós sabe o que é um hambúrguer porque já viu um, e talvez até fez um, ou provou um, ou pelo menos ouviu as pessoas falarem sobre hambúrgueres e entendeu o que são relacionando-os com as coisas que fazemos. conhecer vendo, fazendo e provando. Dado que é assim que se pode saber o que são hambúrgueres, o Robot Reply sugere que coloquemos um computador digital em um corpo de robô, com sensores, como câmeras de vídeo e microfones, e adicionemos efetores, como rodas para se mover e braços para manipular as coisas no mundo. Tal robô – um computador com um corpo – pode fazer o que uma criança faz, aprender vendo e fazendo. A Resposta do Robô sustenta que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. e armas com as quais manipular as coisas no mundo. Tal robô – um computador com um corpo – pode fazer o que uma criança faz, aprender vendo e fazendo. A Resposta do Robô sustenta que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. e armas com as quais manipular as coisas no mundo. Tal robô – um computador com um corpo – pode fazer o que uma criança faz, aprender vendo e fazendo. A Resposta do Robô sustenta que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. Tal robô – um computador com um corpo – pode fazer o que uma criança faz, aprender vendo e fazendo. A Resposta do Robô afirma que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. Tal robô – um computador com um corpo – pode fazer o que uma criança faz, aprender vendo e fazendo. A Resposta do Robô afirma que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. A Resposta do Robô sustenta que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. A Resposta do Robô sustenta que tal computador digital em um corpo de robô, livre da sala, poderia atribuir significados a símbolos e realmente entender a linguagem natural. Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos. Jerry Fodor, Stevan Harnad, Hans Moravec e Georges Rey estão entre aqueles que endossaram versões desta resposta em um momento ou outro. A Resposta do Robô, na verdade, apela para “conteúdo amplo” ou “semântica externalista”. Isso pode concordar com Searle que a sintaxe e as conexões internas isoladas do mundo são insuficientes para a semântica, enquanto sustenta que conexões causais adequadas com o mundo podem fornecer conteúdo aos símbolos internos.

Na época em que Searle pressionava o CRA, muitos na filosofia da linguagem e da mente estavam reconhecendo a importância das conexões causais com o mundo como fonte de significado ou referência para palavras e conceitos. Hilary Putnam 1981 argumentou que um cérebro em uma cuba, isolado do mundo, poderia falar ou pensar em um idioma que soasse como o inglês, mas não seria o inglês – portanto, um cérebro em uma cuba não poderia se perguntar se era um cérebro em uma cuba (devido ao seu isolamento sensorial, suas palavras “cérebro” e “cuba” não se referem a cérebros ou cubas). A visão de que o significado era determinado por conexões com o mundo tornou-se difundida. Searle resistiu a essa virada para fora e continuou a pensar no significado como algo subjetivo e conectado com a consciência.

Uma visão relacionada de que as mentes são mais bem compreendidas como incorporadas ou incorporadas ao mundo ganhou muitos adeptos desde a década de 1990, contra as intuições solipsistas cartesianas. Organismos dependem de características ambientais para o sucesso de seu comportamento. Então, se alguém considera a mente um sistema de processamento de símbolos, com os símbolos obtendo seu conteúdo de conexões sensoriais com o mundo, ou um sistema não-simbólico que consegue ser incorporado em um ambiente particular, a importância das coisas fora da cabeça tem venha à tona. Portanto, muitos simpatizam com alguma forma da Resposta do Robô: um sistema computacional pode entender, desde que esteja atuando no mundo. E.

No entanto, Searle não acredita que o argumento da Resposta do Robô à Sala Chinesa seja mais forte do que a Resposta do Sistema. Tudo o que os sensores podem fazer é fornecer entrada adicional ao computador – e será apenas uma entrada sintática. Podemos ver isso fazendo uma mudança paralela no cenário da Sala Chinesa. Suponha que o homem na Sala Chinesa receba, além dos caracteres chineses inseridos por baixo da porta, um fluxo de dígitos binários que aparecem, digamos, em uma fita adesiva em um canto da sala. Os livros de instruções são aumentados para usar os numerais da fita como entrada, juntamente com os caracteres chineses. Sem o conhecimento do homem na sala, os símbolos na fita são a saída digitalizada de uma câmera de vídeo (e possivelmente de outros sensores). Searle argumenta que entradas sintáticas adicionais não farão nada para permitir que o homem associe significados aos caracteres chineses. É apenas mais trabalho para o homem na sala.

Jerry Fodor, Hilary Putnam e David Lewis foram os principais arquitetos da teoria computacional da mente que o argumento mais amplo de Searle ataca. Em sua resposta original de 1980 a Searle, Fodor permite que Searle certamente esteja certo de que “instanciar o mesmo programa que o cérebro não é, por si só, suficiente para ter aquelas atitudes proposicionais características do organismo que tem o cérebro”. Mas Fodor afirma que Searle está errado sobre a resposta do robô. Um computador pode ter atitudes proposicionais se tiver as conexões causais corretas com o mundo – mas essas não são mediadas por um homem sentado na cabeça do robô. Não sabemos quais são as conexões causais corretas. Searle comete a falácia de inferir de “o homenzinho não é a conexão causal correta” para concluir que nenhuma ligação causal seria bem-sucedida. Há evidências empíricas consideráveis ​​de que os processos mentais envolvem “manipulação de símbolos”; Searle não nos dá nenhuma explicação alternativa (isso às vezes é chamado de argumento do “único jogo na cidade” de Fodor para abordagens computacionais). Nas décadas de 1980 e 1990, Fodor escreveu extensivamente sobre quais devem ser as conexões entre um estado cerebral e o mundo para que o estado tenha propriedades intencionais (representacionais), ao mesmo tempo em que enfatizou que o computacionalismo tem limites porque as computações são intrinsecamente locais e, portanto, não podem explicar raciocínio abdutivo.

Em uma peça posterior, “Yin and Yang in the Chinese Room” (em Rosenthal 1991 pp.524–525), Fodor revisa substancialmente sua visão de 1980. Ele se distancia de sua versão anterior da resposta do robô e sustenta, em vez disso, que a “instanciação” deve ser definida de tal forma que o símbolo seja a causa próxima do efeito – nenhum sujeito interveniente em uma sala. Portanto, Searle na sala não é uma instanciação de uma máquina de Turing, e “a configuração de Searle não instancia a máquina que o cérebro instancia”. Ele conclui: “…a configuração de Searle é irrelevante para a alegação de que uma forte equivalência com o cérebro de um falante de chinês é ipso facto suficiente para falar chinês.” Searle diz sobre a mudança de Fodor: “De todas as zilhões de críticas ao argumento da Sala Chinesa, a de Fodor é talvez a mais desesperada. Ele afirma que precisamente porque o homem na sala chinesa se propõe a implementar as etapas do programa de computador, ele não está implementando as etapas do programa de computador. Ele não oferece nenhum argumento para essa afirmação extraordinária”. (em Rosenthal 1991, p. 525)

Em um artigo de 1986, Georges Rey defendeu uma combinação do sistema e da resposta do robô, depois de observar que o Teste de Turing original é insuficiente como teste de inteligência e compreensão, e que o sistema isolado que Searle descreve na sala certamente não é funcionalmente equivalente ao um verdadeiro falante de chinês sentindo e agindo no mundo. Em um segundo olhar de 2002, “Equívocos de Funcionalismo e AI Forte de Searle”, Rey novamente defende o funcionalismo contra Searle, e na forma particular que Rey chama de “teoria representacional-computacional do pensamento – CRTT”. O CRTT não está comprometido em atribuir pensamento a qualquer sistema que passe no Teste de Turing (como a Sala Chinesa). Também não está comprometido com um modelo manual de conversação para entender a linguagem natural. Em vez disso, o CRTT está preocupado com a intencionalidade, natural e artificial (as representações no sistema são semanticamente avaliáveis ​​– elas são verdadeiras ou falsas, portanto têm relação). Searle sela o funcionalismo com o caráter de “caixa preta” do behaviorismo, mas o funcionalismo se preocupa com a forma como as coisas são feitas. Rey esboça “uma mente modesta” – um sistema CRTT que tem percepção, pode fazer inferências dedutivas e indutivas, toma decisões com base em objetivos e representações de como o mundo é e pode processar a linguagem natural convertendo de e para suas representações nativas. Para explicar o comportamento de tal sistema, precisaríamos usar as mesmas atribuições necessárias para explicar o comportamento de um falante normal de chinês. mas o funcionalismo se preocupa com a forma como as coisas são feitas. Rey esboça “uma mente modesta” – um sistema CRTT que tem percepção, pode fazer inferências dedutivas e indutivas, toma decisões com base em objetivos e representações de como o mundo é e pode processar a linguagem natural convertendo de e para suas representações nativas. Para explicar o comportamento de tal sistema, precisaríamos usar as mesmas atribuições necessárias para explicar o comportamento de um falante normal de chinês. mas o funcionalismo se preocupa com a forma como as coisas são feitas. Rey esboça “uma mente modesta” – um sistema CRTT que tem percepção, pode fazer inferências dedutivas e indutivas, toma decisões com base em objetivos e representações de como o mundo é e pode processar a linguagem natural convertendo de e para suas representações nativas. Para explicar o comportamento de tal sistema, precisaríamos usar as mesmas atribuições necessárias para explicar o comportamento de um falante normal de chinês.

Se aprofundarmos a conversa em chinês no contexto da Resposta do Robô, poderemos novamente ver evidências de que a entidade que entende não é o operador dentro da sala. Suponha que perguntemos ao sistema do robô traduções chinesas de “o que você vê?”, Podemos obter a resposta “Meu velho amigo Shakey” ou “Eu vejo você!”. Considerando que, se telefonarmos para Searle na sala e fizermos as mesmas perguntas em inglês, podemos receber “Essas mesmas quatro paredes” ou “esses malditos livros e cadernos de instruções sem fim”. Novamente, isso é evidência de que temos respondentes distintos aqui, um falante de inglês e um falante de chinês, que veem e fazem coisas bem diferentes. Se o robô gigante se enfurece e destrói grande parte de Tóquio, e o tempo todo o inconsciente Searle está apenas seguindo o programa em seus cadernos na sala, Searle não é culpado de homicídio e caos,

Tim Crane discute o argumento da Sala Chinesa em seu livro de 1991, The Mechanical Mind. Ele cita a analogia da sala luminosa de Churchlands, mas depois argumenta que, ao operar a sala, Searle aprenderia o significado do chinês: “…se Searle não apenas memorizasse as regras e os dados, mas também começasse agindo no mundo do povo chinês, então é plausível que ele logo perceberia o que esses símbolos significam.”(127). (Rapaport 2006 faz uma analogia entre Helen Keller e a Sala Chinesa.) Crane parece terminar com uma versão da Resposta do Robô: “O próprio argumento de Searle levanta a questão (na verdade) apenas negando a tese central da IA ​​– que pensar é manipulação formal de símbolos. Mas a suposição de Searle, não obstante, parece-me correta... a resposta adequada ao argumento de Searle é: claro, Searle-in-the-room, ou o quarto sozinho, não consegue entender chinês. Mas se você deixar o mundo exterior ter algum impacto na sala, o significado ou a 'semântica' pode começar a se firmar. Mas é claro que isso admite que o pensamento não pode ser simplesmente manipulação de símbolos”. (129) A ideia de que a aprendizagem fundamenta a compreensão levou ao trabalho em robótica de desenvolvimento (também conhecida como robótica epigenética). Esta área de pesquisa de IA busca replicar as principais habilidades de aprendizado humano, como robôs que veem um objeto de vários ângulos enquanto ouvem em linguagem natural o nome do objeto.

Margaret Boden 1988 também argumenta que Searle erroneamente supõe que os programas são sintaxe pura. Mas os programas provocam a atividade de certas máquinas: “As consequências procedimentais inerentes de qualquer programa de computador dão a ele um ponto de apoio na semântica, onde a semântica em questão não é denotacional, mas causal”. (250) Assim, um robô pode ter poderes causais que o permitam se referir a um hambúrguer.

Stevan Harnad também considera importantes nossas capacidades sensoriais e motoras: “Quem pode dizer que o Teste de Turing, seja conduzido em chinês ou em qualquer outro idioma, poderia ser aprovado sem operações que extraíssem nossas capacidades sensoriais, motoras e outras capacidades cognitivas superiores? também? Onde começa a capacidade de compreender o chinês e termina o resto da nossa competência mental?” Harnad acredita que as funções simbólicas devem ser fundamentadas em funções “robóticas” que conectam um sistema com o mundo. E ele acha que isso conta contra relatos simbólicos de mentalidade, como o de Jerry Fodor e, suspeita-se, a abordagem de Roger Schank que era o alvo original de Searle. Harnad 2012 (Outros recursos da Internet) argumenta que o CRA mostra que mesmo com um robô com símbolos fundamentados no mundo externo, ainda falta algo: sentimento,

No entanto, Ziemke 2016 argumenta que uma incorporação robótica com sistemas em camadas de regulação corporal pode fundamentar a emoção e o significado, e Seligman 2019 argumenta que abordagens “baseadas na percepção” para o processamento de linguagem natural (PNL) têm o “potencial de exibir intencionalidade e, portanto, promover uma semântica verdadeiramente significativa que, na visão de Searle e outros céticos, está intrinsecamente além da capacidade dos computadores”.

4.3 Resposta do Brain Simulator
Considere um computador que opera de maneira bastante diferente do programa de IA usual, com scripts e operações em sequências de símbolos semelhantes a sentenças. A resposta do Brain Simulator nos pede para supor que, em vez disso, o programa simule a sequência real de disparos nervosos que ocorrem no cérebro de um falante nativo da língua chinesa quando essa pessoa entende chinês – cada nervo, cada disparo. Como o computador funciona da mesma maneira que o cérebro de um falante nativo de chinês, processando informações da mesma maneira, ele entenderá o chinês. Paul e Patricia Churchland apresentaram uma resposta nesse sentido, discutida abaixo.

Em resposta a isso, Searle argumenta que não faz diferença. Ele sugere uma variação do cenário do simulador de cérebro: suponha que na sala o homem tenha um enorme conjunto de válvulas e canos de água, no mesmo arranjo dos neurônios no cérebro de um falante nativo de chinês. O programa agora diz ao homem quais válvulas abrir em resposta à entrada. Searle afirma que é óbvio que não haveria compreensão do chinês. (Observe, no entanto, que a base para essa afirmação não é mais simplesmente que o próprio Searle não entenderia chinês - parece claro que agora ele está apenas facilitando a operação causal do sistema e, portanto, contamos com nossa intuição leibniziana de que as obras hidráulicas não funcionam. 'não entendo (ver também Maudlin 1989).) Searle conclui que uma simulação da atividade cerebral não é a coisa real.

No entanto, seguindo Pylyshyn 1980, Cole e Foelber 1984, Chalmers 1996, podemos nos perguntar sobre sistemas híbridos. Pylyshyn escreve:

Se mais e mais células em seu cérebro fossem substituídas por chips de circuito integrado, programados de forma a manter a função de entrada-saída de cada unidade idêntica à da unidade que está sendo substituída, você provavelmente apenas manteria direito de falar exatamente como você está fazendo agora, exceto que você acabaria por parar de significar qualquer coisa com isso. O que nós, observadores de fora, poderíamos considerar palavras, tornar-se-iam para você apenas certos ruídos que os circuitos faziam com que você fizesse.
Esses experimentos mentais de ciborguização podem ser vinculados à Sala Chinesa. Suponha que Otto tenha uma doença neural que causa a falha de um dos neurônios do meu cérebro, mas os cirurgiões instalam um minúsculo neurônio artificial controlado remotamente, um sínron, ao lado de seu neurônio desativado. O controle do neurônio de Otto é feito por John Searle na Sala Chinesa, sem o conhecimento de Searle e Otto. Fios minúsculos conectam o neurônio artificial às sinapses no corpo celular de seu neurônio desativado. Quando seu neurônio artificial é estimulado por neurônios que fazem sinapse em seu neurônio desativado, uma luz se acende na Sala Chinesa. Searle então manipula algumas válvulas e interruptores de acordo com um programa. Isso, via link de rádio, faz com que o neurônio artificial de Otto libere neurotransmissores de suas minúsculas vesículas artificiais. Se a atividade programada de Searle fizer com que o neurônio artificial de Otto se comporte exatamente como seu neurônio natural desativado, o comportamento do resto de seu sistema nervoso permanecerá inalterado. Infelizmente, a doença de Otto progride; mais neurônios são substituídos por sínrons controlados por Searle.Por hipótese, o resto do mundo não notará a diferença; vai Otto? Se assim for, quando? E porque?

Sob a rubrica “The Combination Reply”, Searle também considera um sistema com as características dos três anteriores: um robô com um cérebro digital simulando um computador em seu crânio, de modo que o sistema como um todo se comporte de maneira indistinguível de um humano. Como a entrada normal para o cérebro vem dos órgãos dos sentidos, é natural supor que a maioria dos defensores do Brain Simulator Reply tenha em mente essa combinação de simulação cerebral, Robot e Systems Reply. Alguns (por exemplo, Rey 1986) argumentam que é razoável atribuir intencionalidade a tal sistema como um todo. Searle concorda que seria realmente razoável atribuir entendimento a tal sistema andróide – mas apenas enquanto você não souber como ele funciona. Assim que você souber a verdade - é um computador, manipulando incompreensivelmente símbolos com base na sintaxe,

(Supõe-se que isso seria verdade mesmo se fosse o cônjuge, com quem construímos um relacionamento para toda a vida, que revelou esconder um segredo de silício. Histórias de ficção científica, incluindo episódios da série de televisão de Rod Serling, The Twilight Zone , têm baseado em tais possibilidades (o rosto da amada se revela para revelar a terrível verdade do andróide); no entanto, Steven Pinker (1997) menciona um episódio em que o segredo do andróide era conhecido desde o início, mas o protagonista desenvolveu um relacionamento amoroso com o andróide.)

Em seu décimo aniversário, o argumento da Sala Chinesa foi apresentado no periódico científico geral Scientific American. Liderando a oposição ao artigo principal de Searle naquela edição estavam os filósofos Paul e Patricia Churchland. Os Churchlands concordam com Searle que o Chinese Room não entende chinês, mas sustentam que o próprio argumento explora nossa ignorância dos fenômenos cognitivos e semânticos. Eles levantam um caso paralelo de “A Sala Luminosa” onde alguém agita um ímã e argumenta que a ausência de luz visível resultante mostra que a teoria eletromagnética de Maxwell é falsa. Os Churchlands defendem uma visão do cérebro como um sistema conexionista, um transformador de vetores, não um sistema que manipula símbolos de acordo com regras sensíveis à estrutura. O sistema na Sala Chinesa usa estratégias computacionais erradas. Assim, eles concordam com Searle contra a IA tradicional, mas presumivelmente endossariam o que Searle chama de “a resposta do simulador cerebral”.

Em seu livro de 1991, Microcognição. Andy Clark afirma que Searle está certo de que um computador executando o programa de Schank não sabe nada sobre restaurantes, “pelo menos se por 'saber' queremos dizer algo como 'entender'”. Mas Searle acha que isso se aplicaria a qualquer modelo computacional, enquanto Clark, como os Churchlands, sustenta que Searle está errado sobre os modelos conexionistas. O interesse de Clark está, portanto, na resposta do simulador cerebral. O cérebro pensa em virtude de suas propriedades físicas. Quais propriedades físicas do cérebro são importantes? Clark responde que o que é importante sobre os cérebros são “subestruturas variáveis ​​e flexíveis” que faltam nos sistemas convencionais de IA. Mas isso não significa que o computacionalismo ou o funcionalismo sejam falsos. Depende de que nível você considera as unidades funcionais. Clark defende o “microfuncionalismo” – deve-se olhar para uma descrição funcional refinada, por exemplo nível da rede neural. Clark cita William Lycan com aprovação contra a objeção de ausência de qualia de Block – sim, pode haver ausência de qualia, se as unidades funcionais forem ampliadas. Mas isso não constitui uma refutação do funcionalismo em geral. Portanto, as opiniões de Clark não são diferentes das dos Churchlands, admitindo que Searle está certo sobre Schank e os sistemas de processamento de nível simbólico, mas sustentando que ele está errado sobre os sistemas conexionistas.

Da mesma forma, Ray Kurzweil (2002) argumenta que o argumento de Searle poderia ser revertido para mostrar que os cérebros humanos não podem entender – o cérebro consegue manipular concentrações de neurotransmissores e outros mecanismos que são em si sem sentido. Em crítica à resposta de Searle ao Brain Simulator Reply, Kurzweil diz: “Então, se ampliarmos a Sala Chinesa de Searle para ser a 'sala' bastante grande que precisa ser, quem pode dizer que todo o sistema de cem trilhões de pessoas simulando um Cérebro chinês que sabe chinês não é consciente? Certamente, seria correto dizer que tal sistema conhece chinês. E não podemos dizer que não é consciente, assim como não podemos dizer isso sobre qualquer outro processo. Não podemos conhecer a experiência subjetiva de outra entidade…”

4.4 A Resposta das Outras Mentes
Relacionado ao anterior está a resposta de The Other Minds: “Como você sabe que outras pessoas entendem chinês ou qualquer outra coisa? Apenas por seu comportamento. Agora, o computador pode passar nos testes comportamentais tão bem quanto eles (em princípio), então se você vai atribuir cognição a outras pessoas, você deve, em princípio, também atribuí-la aos computadores”.

A resposta de Searle (1980) a isso é muito curta:

O problema nesta discussão não é como sei que outras pessoas têm estados cognitivos, mas sim o que estou atribuindo a elas quando atribuo estados cognitivos a elas. A força do argumento é que não poderia ser apenas processos computacionais e seus resultados porque os processos computacionais e seus resultados podem existir sem o estado cognitivo. Não é resposta a esse argumento fingir anestesia. Nas 'ciências cognitivas' pressupõe-se a realidade e cognoscibilidade do mental da mesma forma que nas ciências físicas tem-se que pressupor a realidade e cognoscibilidade dos objetos físicos.
Os críticos sustentam que, se a evidência que temos de que os humanos entendem é a mesma que a evidência que podemos ter de que um alienígena visitante extraterrestre entende, que é a mesma evidência de que um robô entende, as pressuposições que podemos fazer no caso de nossa própria espécie não é relevante, pois as pressuposições às vezes são falsas. Por razões semelhantes, Turing, ao propor o Teste de Turing, preocupa-se especificamente com nossos pressupostos e chauvinismo. Se as razões para as pressuposições relativas aos humanos são pragmáticas, pois nos permitem prever o comportamento dos humanos e interagir efetivamente com eles, talvez a pressuposição possa ser aplicada igualmente aos computadores (considerações semelhantes são feitas por Dennett, em suas discussões sobre o que ele chama de Postura Intencional).

Searle levanta a questão do que estamos atribuindo ao atribuir compreensão a outras mentes, dizendo que é mais do que disposições comportamentais complexas. Para Searle, o adicional parece ser certos estados de consciência, como pode ser visto em seu resumo de 2010 das conclusões do CRA. Terry Horgan (2013) endossa esta afirmação: “a verdadeira moral do experimento de pensamento do quarto chinês de Searle é que a genuína intencionalidade original requer a presença de estados internos com caráter fenomenal intrínseco que é inerentemente intencional…” Mas essa vinculação da compreensão à consciência fenomenal levanta uma questão série de questões.

Atribuímos uma compreensão limitada da linguagem a crianças, cachorros e outros animais, mas não está claro se estamos ipso facto atribuindo estados invisíveis de consciência subjetiva – o que sabemos sobre os estados ocultos de criaturas exóticas? Ludwig Wittgenstein (o argumento da linguagem privada) e seus seguidores defenderam pontos semelhantes. Surgem possibilidades qualia alteradas, análogas ao espectro invertido: suponha que eu pergunte “qual é a soma de 5 e 7” e você responda “a soma de 5 e 7 é 12”, mas ao ouvir minha pergunta você teve a experiência consciente de ouvindo e entendendo “qual é a soma de 10 e 14”, embora você estivesse nos estados computacionais apropriados para produzir a soma correta e assim dissesse “12”. Existem certos estados conscientes que são “corretos” para certos estados funcionais? As considerações de Wittgenstein parecem ser de que o estado subjetivo é irrelevante, na melhor das hipóteses epifenomenal, se um usuário de linguagem exibe comportamento linguístico apropriado. Afinal, aprendemos a linguagem com base em nossas respostas abertas, não em nossas qualia. O sábio matemático Daniel Tammet relata que quando ele gera a expansão decimal de pi para milhares de dígitos, ele experimenta cores que revelam o próximo dígito, mas mesmo aqui pode ser que o desempenho de Tennant provavelmente não seja produzido pelas cores que ele experimenta, mas sim por computação neural inconsciente. A possível importância dos estados subjetivos é considerada mais adiante na seção sobre Intencionalidade, abaixo. se um usuário de idioma exibe comportamento linguístico apropriado. Afinal, aprendemos a linguagem com base em nossas respostas abertas, não em nossas qualia. O sábio matemático Daniel Tammet relata que quando ele gera a expansão decimal de pi para milhares de dígitos, ele experimenta cores que revelam o próximo dígito, mas mesmo aqui pode ser que o desempenho de Tennant provavelmente não seja produzido pelas cores que ele experimenta, mas sim por computação neural inconsciente. A possível importância dos estados subjetivos é considerada mais adiante na seção sobre Intencionalidade, abaixo. se um usuário de idioma exibe comportamento linguístico apropriado. Afinal, aprendemos a linguagem com base em nossas respostas abertas, não em nossas qualia. O sábio matemático Daniel Tammet relata que quando ele gera a expansão decimal de pi para milhares de dígitos, ele experimenta cores que revelam o próximo dígito, mas mesmo aqui pode ser que o desempenho de Tennant provavelmente não seja produzido pelas cores que ele experimenta, mas sim por computação neural inconsciente. A possível importância dos estados subjetivos é considerada mais adiante na seção sobre Intencionalidade, abaixo. mas mesmo aqui pode ser que o desempenho de Tennant provavelmente não seja produzido pelas cores que ele experimenta, mas sim por computação neural inconsciente. A possível importância dos estados subjetivos é considerada mais adiante na seção sobre Intencionalidade, abaixo. mas mesmo aqui pode ser que o desempenho de Tennant provavelmente não seja produzido pelas cores que ele experimenta, mas sim por computação neural inconsciente. A possível importância dos estados subjetivos é considerada mais adiante na seção sobre Intencionalidade, abaixo.

Nos 30 anos desde o CRA, houve interesse filosófico em zumbis – criaturas que se parecem e se comportam como humanos normais, incluindo comportamento linguístico, mas não têm consciência subjetiva. Uma dificuldade para afirmar que os estados subjetivos de consciência são cruciais para a compreensão do significado surgirá nesses casos de qualia ausentes: não podemos dizer a diferença entre zumbis e não-zumbis e, portanto, no relato de Searle, não podemos dizer a diferença entre aqueles que realmente entendem inglês e aqueles que não entendem. E se você e eu não podemos dizer a diferença entre aqueles que entendem a linguagem e os zumbis que se comportam como eles, mas não o fazem, então nenhum fator de seleção na história da evolução humana pode ser - para predadores, presas e parceiros, zumbis e verdadeiros entendedores, com a experiência consciente “certa”, têm sido indistinguíveis. Mas então parece haver uma distinção sem diferença. Em qualquer caso, a resposta curta de Searle à Resposta das Outras Mentes pode ser curta demais.

Descartes argumentou famosamente que a fala era suficiente para atribuir mentes e consciência a outros, e argumentou infamemente que era necessário. Turing estava de fato endossando a condição de suficiência de Descartes, pelo menos para a inteligência, enquanto substituía o comportamento linguístico oral por escrito. Uma vez que a maioria de nós usa o diálogo como uma condição suficiente para atribuir compreensão, o argumento de Searle, que sustenta que a fala é uma condição suficiente para atribuir compreensão aos humanos, mas não para qualquer coisa que não compartilhe nossa biologia, uma explicação parece ser necessária para o que adicionalmente está sendo atribuído e o que pode justificar a atribuição adicional. Além disso, se ser con-específico é fundamental na conta de Searle, uma questão natural surge sobre quais circunstâncias nos justificariam em atribuir compreensão (ou consciência) a alienígenas extraterrestres que não compartilham nossa biologia? Ofender ETs retendo atribuições de compreensão até depois de fazer uma autópsia pode ser arriscado.

Hans Moravec, diretor do laboratório de robótica da Carnegie Mellon University e autor de Robot: Mere Machine to Transcendent Mind,argumenta que a posição de Searle meramente reflete intuições da filosofia tradicional da mente que estão fora de sintonia com a nova ciência cognitiva. Moravec endossa uma versão da resposta de Other Minds. Faz sentido atribuir intencionalidade às máquinas pelas mesmas razões que faz sentido atribuí-las aos humanos; sua “posição interpretativa” é semelhante às visões de Daniel Dennett. Moravec continua observando que uma das coisas que atribuímos aos outros é a capacidade de fazer atribuições de intencionalidade, e então fazemos tais atribuições a nós mesmos. É essa autorrepresentação que está no cerne da consciência. Essas capacidades parecem ser independentes de implementação e, portanto, possíveis para alienígenas e computadores adequadamente programados.

Como vimos, a razão pela qual Searle pensa que podemos desconsiderar a evidência no caso de robôs e computadores é que sabemos que seu processamento é sintático, e esse fato supera todas as outras considerações. De fato, Searle acredita que este é o ponto mais amplo que a Sala Chinesa meramente ilustra. Esse ponto maior é abordado na seção Sintaxe e semântica abaixo.

4.5 A resposta da intuição
Muitas respostas ao argumento da Sala Chinesa observaram que, como com Leibniz' Mill, o argumento parece ser baseado na intuição: a intuição de que um computador (ou o homem na sala) não pode pensar ou ter entendimento. Por exemplo, Ned Block (1980), em seu comentário original do BBS, diz que “o argumento de Searle depende de sua força de intuições que certas entidades não pensam”. Mas, argumenta Block, (1) as intuições às vezes podem e devem ser superadas e (2) talvez precisemos alinhar nosso conceito de compreensão com uma realidade na qual certos robôs de computador pertencem ao mesmo tipo natural que os humanos. Da mesma forma, Margaret Boden (1988) aponta que não podemos confiar em nossas intuições não treinadas sobre como a mente depende da matéria; desenvolvimentos na ciência podem mudar nossas intuições. De fato, a eliminação do viés em nossas intuições foi precisamente o que motivou Turing (1950) a propor o Teste de Turing, um teste que era cego para o caráter físico do sistema respondendo às perguntas. Alguns dos críticos de Searle, na verdade, argumentam que ele apenas empurrou a confiança na intuição de volta para a sala.

Por exemplo, pode-se sustentar que, apesar da intuição de Searle de que ele não entenderia chinês enquanto estivesse na sala, talvez ele estivesse enganado e entendesse, embora inconscientemente. Hauser (2002) acusa Searle de viés cartesiano em sua inferência de “me parece bastante óbvio que não entendo nada” para a conclusão de que realmente não entendo nada. Normalmente, se alguém entende inglês ou chinês, sabe que entende – mas não necessariamente. Searle carece da consciência introspectiva normal de compreensão – mas isso, embora anormal, não é conclusivo.

Os críticos do CRA observam que nossas intuições sobre inteligência, compreensão e significado podem não ser confiáveis. Com relação ao significado, Wakefield 2003, seguindo Block 1998, defende o que Wakefield chama de “a objeção essencialista” ao CRA, ou seja, que uma explicação computacional do significado não é a análise de conceitos comuns e suas intuições relacionadas. Em vez disso, estamos construindo uma teoria científica do significado que pode exigir a revisão de nossas intuições. Como teoria, obtém sua evidência de seu poder explicativo, não de sua concordância com intuições pré-teóricas (no entanto, o próprio Wakefield argumenta que as explicações computacionais do significado são afligidas por uma indeterminação perniciosa (pp. 308ff)).

Outros críticos que se concentram no papel das intuições no CRA argumentam que nossas intuições em relação à inteligência e à compreensão também podem não ser confiáveis ​​e talvez incompatíveis mesmo com a ciência atual. No que diz respeito à compreensão, Steven Pinker, em How the Mind Works (1997), afirma que “… Searle está meramente explorando fatos sobre a palavra inglesa “entender” .…. As pessoas relutam em usar a palavra, a menos que certas condições estereotipadas se apliquem…” Mas, afirma Pinker, nada cientificamente falando está em jogo. Pinker contesta o apelo de Searle aos “poderes causais do cérebro” observando que o locus aparente dos poderes causais são os “padrões de interconectividade que realizam o processamento correto da informação”. Pinker termina sua discussão citando uma história de ficção científica na qual os alienígenas, anatomicamente muito diferentes dos humanos, não conseguem acreditar que os humanos pensam quando descobrem que nossas cabeças estão cheias de carne. As intuições dos Aliens não são confiáveis ​​– presumivelmente as nossas também podem ser.

Claramente, o CRA ativa o que é necessário para entender a linguagem. Schank 1978 esclarece sua afirmação sobre o que ele pensa que seus programas podem fazer: “Por 'entender', queremos dizer que o SAM [um de seus programas] pode criar uma cadeia causal vinculada de conceitualizações que representam o que aconteceu em cada história.” Esta é uma compreensão nuançada de “compreensão”, enquanto o experimento mental da Sala Chinesa não se baseia em uma compreensão técnica de “compreensão”, mas sim em intuições sobre nossa competência comum quando entendemos uma palavra como “hambúrguer”. De fato, em 2015, Schank se distancia dos fracos sentidos de “entender”, sustentando que nenhum computador pode “entender quando você diz algo” e que o WATSON da IBM “não sabe o que está dizendo”. O programa de Schank pode acertar os links, mas sem dúvida não sabe quais são as entidades vinculadas. Se depende ou não de quais são os conceitos, consulte a seção 5.1. Além disso, é possível que, quando se trata de atribuir compreensão da linguagem, tenhamos padrões diferentes para coisas diferentes – mais relaxados para cães e crianças. Algumas coisas entendem uma linguagem “un poco”. Searle (1980) admite que existem graus de compreensão, mas diz que tudo o que importa é que existem casos claros de não compreensão, e os programas de IA são um exemplo: “A compreensão do computador não é apenas (como minha compreensão do alemão) parcial ou incompleto; é zero”. Searle (1980) admite que existem graus de compreensão, mas diz que tudo o que importa é que existem casos claros de não compreensão, e os programas de IA são um exemplo: “A compreensão do computador não é apenas (como minha compreensão do alemão) parcial ou incompleto; é zero”. Searle (1980) admite que existem graus de compreensão, mas diz que tudo o que importa é que existem casos claros de não compreensão, e os programas de IA são um exemplo: “A compreensão do computador não é apenas (como minha compreensão do alemão) parcial ou incompleto; é zero”.

Alguns defensores da IA ​​também estão preocupados em como nossa compreensão do entendimento se relaciona com o argumento da Sala Chinesa. Em seu artigo “Uma sala chinesa que entende”, os pesquisadores de IA Simon e Eisenstadt (2002) argumentam que, enquanto Searle refuta a “IA lógica forte”, a tese de que um programa que passa no Teste de Turing necessariamente entenderá, o argumento de Searle não impugna “Inteligência Empírica ” . IA forte” – a tese de que é possível programar um computador que satisfaça de forma convincente os critérios ordinários de compreensão. Eles sustentam, entretanto, que é impossível resolver essas questões “sem empregar uma definição do termo 'compreender' que possa fornecer um teste para julgar se a hipótese é verdadeira ou falsa”. Eles citam Word and Object de WVO Quinecomo mostrando que há sempre incerteza empírica em atribuir compreensão aos humanos. A Sala Chinesa é um truque do Clever Hans (Clever Hans era um cavalo que parecia conseguir as respostas para questões aritméticas simples, mas foi descoberto que Hans podia detectar pistas inconscientes de seu treinador). Da mesma forma, o homem na sala não entende chinês e pode ser exposto ao observá-lo de perto. (Simon e Eisenstadt não explicam exatamente como isso seria feito, ou como isso afetaria o argumento.) Citando o trabalho de Rudolf Carnap, Simon e Eisenstadt argumentam que entender não é apenas exibir certo comportamento, mas usar “intensões ” que determinam extensões e que se pode ver em programas reais que eles usam as intenções apropriadas. Eles discutem três programas reais de IA, e defender várias atribuições de mentalidade a eles, incluindo compreensão, e concluir que os computadores entendem; eles aprendem “intensões associando palavras e outras estruturas lingüísticas com suas denotações, conforme detectadas por meio de estímulos sensoriais”. E como podemos ver exatamente como as máquinas funcionam, “é, de fato, mais fácil estabelecer que uma máquina exibe entendimento do que estabelecer que um humano exibe entendimento…”. Assim, eles concluem, a evidência para IA forte empírica é esmagadora. mais fácil estabelecer que uma máquina exibe compreensão do que estabelecer que um ser humano exibe compreensão...” Assim, eles concluem, a evidência para IA forte empírica é esmagadora. mais fácil estabelecer que uma máquina exibe compreensão do que estabelecer que um ser humano exibe compreensão...” Assim, eles concluem, a evidência para IA forte empírica é esmagadora.

Da mesma forma, Daniel Dennett em sua resposta original de 1980 ao argumento de Searle chamou de “uma bomba de intuição”, um termo que ele criou ao discutir o CRA com Hofstader. Sharvy 1983 ecoa a reclamação. A visão ponderada de Dennett (2013) é que o CRA é “claramente um argumento falacioso e enganoso …”. (pág. 320). Paul Thagard (2013) propõe que para cada experimento de pensamento em filosofia existe um experimento de pensamento igual e oposto. Thagard sustenta que as intuições não são confiáveis, e o CRA é um exemplo (e que, de fato, o CRA já foi refutado pela tecnologia de carros robóticos autônomos). Dennett elaborou preocupações sobre nossas intuições em relação à inteligência. Dennett 1987 (“Pensamento Rápido”) expressou preocupação sobre a velocidade lenta em que a Sala Chinesa operaria, e vários outros comentaristas se juntaram a ele, incluindo Tim Maudlin, David Chalmers e Steven Pinker. O operador da Sala Chinesa pode eventualmente produzir respostas apropriadas às perguntas chinesas. Mas os pensadores lentos são estúpidos, não inteligentes – e na selva, eles podem acabar mortos. Dennett argumenta que “velocidade … é 'essencial' para a inteligência. Se você não consegue descobrir as porções relevantes do ambiente em mudança rápido o suficiente para se defender sozinho, você não é praticamente inteligente, por mais complexo que seja” (326). Assim, Dennett relativiza a inteligência à velocidade de processamento relativa ao ambiente atual. Dennett argumenta que “velocidade … é 'essencial' para a inteligência. Se você não consegue descobrir as porções relevantes do ambiente em mudança rápido o suficiente para se defender sozinho, você não é praticamente inteligente, por mais complexo que seja” (326). Assim, Dennett relativiza a inteligência à velocidade de processamento relativa ao ambiente atual. Dennett argumenta que “velocidade … é 'essencial' para a inteligência. Se você não consegue descobrir as porções relevantes do ambiente em mudança rápido o suficiente para se defender sozinho, você não é praticamente inteligente, por mais complexo que seja” (326). Assim, Dennett relativiza a inteligência à velocidade de processamento relativa ao ambiente atual.

Tim Maudlin (1989) discorda. Maudlin considera o problema da escala de tempo apontado por outros escritores e conclui, contra Dennett, que a extrema lentidão de um sistema computacional não viola nenhuma condição necessária do pensamento ou da consciência. Além disso, a principal afirmação de Searle é sobre compreensão, não inteligência ou perspicácia. Se encontrássemos extraterrestres que pudessem processar informações mil vezes mais rápido do que nós, parece que isso não mostraria nada sobre nossa própria capacidade de entender os idiomas que falamos.

Steven Pinker (1997) também sustenta que Searle depende de intuições não treinadas. Pinker endossa o contra-exemplo de Churchlands (1990) de um experimento de pensamento análogo de acenar um ímã e não gerar luz, observando que esse resultado não refutaria a teoria de Maxwell de que a luz consiste em ondas eletromagnéticas. Pinker sustenta que a questão-chave é a velocidade: “O experimento de pensamento desacelera as ondas a um alcance em que nós, humanos, não as vemos mais como luz. Ao confiar em nossas intuições no experimento mental, concluímos falsamente que ondas rápidas também não podem ser luz. Da mesma forma, Searle desacelerou as computações mentais a um nível em que nós, humanos, não pensamos mais nisso como compreensão (já que a compreensão é normalmente muito mais rápida)” (94-95). Howard Gardiner, um defensor das conclusões de Searle sobre a sala, faz um ponto semelhante sobre a compreensão. Gardiner aborda o argumento da Sala Chinesa em seu livroA Nova Ciência da Mente (1985, 171-177). Gardiner considera todas as respostas padrão ao argumento da Sala Chinesa e conclui que Searle está correto sobre a sala: “…a palavra entender foi indevidamente ampliada no caso da sala chinesa…”. (175).

Assim, vários neste grupo de críticos argumentam que a velocidade afeta nossa disposição de atribuir inteligência e compreensão a um sistema lento, como o da Sala Chinesa. O resultado pode ser simplesmente que nossas intuições em relação à Sala Chinesa não são confiáveis ​​e, portanto, o homem na sala, ao implementar o programa, pode entender chinês, apesar das intuições em contrário (Maudlin e Pinker). Ou pode ser que a lentidão marque uma diferença crucial entre a simulação na sala e o que um computador rápido faz, de forma que o homem não é inteligente enquanto o sistema do computador é (Dennett).

5. As Questões Filosóficas Maiores
5.1 Sintaxe e Semântica
Searle acredita que o argumento da Sala Chinesa apóia um ponto mais amplo, o que explica o fracasso da Sala Chinesa em produzir compreensão. Searle argumentou que os programas implementados por computadores são apenas sintáticos. As operações do computador são “formais” no sentido de que respondem apenas à forma física das sequências de símbolos, não ao significado dos símbolos. As mentes, por outro lado, têm estados com significado, conteúdos mentais. Associamos significados às palavras ou sinais na linguagem. Respondemos aos sinais por causa de seu significado, não apenas por sua aparência física. Em suma, nós entendemos. Mas, e de acordo com Searle este é o ponto chave, “a sintaxe não é por si só suficiente para, nem constitutiva da, semântica”. Portanto, embora os computadores possam manipular a sintaxe para produzir respostas apropriadas à entrada de linguagem natural,

Searle (1984) apresenta um argumento de três premissas de que, como a sintaxe não é suficiente para a semântica, os programas não podem produzir mentes.

Os programas são puramente formais (sintáticos).
As mentes humanas têm conteúdos mentais (semântica).
A sintaxe por si só não é constitutiva nem suficiente para o conteúdo semântico.
Portanto, os programas por si só não são constitutivos nem suficientes para as mentes.
O próprio experimento mental da Sala Chinesa é o suporte para a terceira premissa. A alegação de que a manipulação sintática não é suficiente para o significado ou o pensamento é uma questão significativa, com implicações mais amplas do que a IA ou atribuições de compreensão. Teorias proeminentes da mente sustentam que a cognição humana geralmente é computacional. De uma forma, sustenta-se que o pensamento envolve operações em símbolos em virtude de suas propriedades físicas. Em uma conta conexionista alternativa, as computações estão em estados “subsimbólicos”. Se Searle estiver certo, não apenas a IA forte, mas também essas principais abordagens para entender a cognição humana estão equivocadas.

Como vimos, Searle sustenta que o cenário da Sala Chinesa mostra que não se pode obter semântica apenas da sintaxe. Em um sistema lógico simbólico, uma espécie de linguagem artificial, são dadas regras para a sintaxe. Uma semântica, se houver, vem depois. O lógico especifica o conjunto básico de símbolos e algumas regras para manipular strings para produzir novos. Essas regras são puramente sintáticas – elas são aplicadas a sequências de símbolos apenas em virtude de sua sintaxe ou forma. Uma semântica, se houver, para o sistema de símbolos deve ser fornecida separadamente. E se alguém deseja mostrar que existem relações adicionais interessantes entre as operações sintáticas e a semântica, como que as manipulações de símbolos preservam a verdade, deve-se fornecer às vezes metaprovas complexas para mostrar isso. Portanto, aparentemente, a semântica é bastante independente da sintaxe para linguagens artificiais, e não se pode obter semântica apenas da sintaxe. “Símbolos formais por si só nunca podem ser suficientes para conteúdos mentais, porque os símbolos, por definição, não têm significado (ou interpretação, ou semântica), exceto na medida em que alguém fora do sistema os dá” (Searle 1989, 45).

A identificação de Searle do significado com a interpretação nesta passagem é importante. O ponto de Searle é claramente verdadeiro para os sistemas formais causalmente inertes dos lógicos. Uma interpretação semântica deve ser dada a esses símbolos por um lógico. Quando passamos de sistemas formais para sistemas computacionais, a situação é mais complexa. Como muitos dos críticos de Searle (por exemplo, Cole 1984, Dennett 1987, Boden 1988 e Chalmers 1996) observaram, um computador executando um programa não é o mesmo que “somente a sintaxe”. Um computador é um sistema causal eletrônico extremamente complexo. As mudanças de estado no sistema são físicas. Alguém pode interpretaros estados físicos, por exemplo, tensões, como sintáticos 1's e 0's, mas a realidade intrínseca é eletrônica e a sintaxe é “derivada”, um produto de interpretação. Os estados são especificados sintaticamente pelos programadores, mas quando implementados em uma máquina em execução, eles são estados eletrônicos de um sistema causal complexo embutido no mundo real. Isso é bem diferente dos sistemas formais abstratos que os lógicos estudam. Dennett observa que nenhum “programa de computador por si só” (linguagem de Searle) – por exemplo, um programa em uma prateleira – pode causar qualquer coisa, mesmo uma simples adição, muito menos estados mentais. O programa deve estar em execução. Chalmers (1996) oferece uma paródia na qual se argumenta que as receitas são sintáticas, a sintaxe não é suficiente para a quebradiça, os bolos são quebradiços, então a implementação de uma receita não é suficiente para fazer um bolo. A implementação faz toda a diferença; uma entidade abstrata (receita, programa) determina os poderes causais de um sistema físico embutido no nexo causal maior do mundo.

Dennett (1987) resume a questão: “A visão de Searle, então, chega a isso: pegue um objeto material (qualquer objeto material) que não tenha o poder de causar fenômenos mentais; você não pode transformá-lo em um objeto que tenha o poder de produzir fenômenos mentais simplesmente programando-o – reorganizando as dependências condicionais das transições entre seus estados”. A visão de Dennett é oposta: a programação “é precisamente o que poderia dar uma mente a algo”. Mas Dennett afirma que, de fato, é “empiricamente improvável que os tipos certos de programas possam ser executados em qualquer coisa que não seja cérebro humano orgânico” (325-6).

Uma outra complicação relacionada é que não está claro se os computadores executam operações sintáticas exatamente da mesma forma que um ser humano – não está claro se um computador entende sintaxe ou operações sintáticas. Um computador não sabe que está manipulando 1s e 0s. Um computador não reconhece que suas strings de dados binários têm uma certa forma e, portanto, certas regras sintáticas podem ser aplicadas a elas, ao contrário do homem dentro da Sala Chinesa. Dentro de um computador, não há nada que literalmente leia os dados de entrada ou que “sabe” o que são símbolos. Em vez disso, existem milhões de transistores que mudam de estado. Uma sequência de tensões faz com que as operações sejam executadas. Nós, humanos, podemos escolher interpretar essas tensões como numerais binários e as mudanças de tensão como operações sintáticas, mas um computador não interpreta suas operações como sintáticas ou de qualquer outra forma. Portanto, talvez um computador não precise passar da sintaxe para a semântica à qual Searle se opõe; ele precisa passar de conexões causais complexas para semântica. Além disso, talvezqualquer sistema causal é descrito como executando operações sintáticas – se interpretarmos um quadrado claro como “0” lógico e um quadrado escuro como “1” lógico, então uma torradeira de cozinha pode ser descrita como um dispositivo que reescreve “0” lógicos como lógico “1”s. Mas não há nenhum problema filosófico em passar da sintaxe para o café da manhã.

Na década de 1990, Searle começou a usar considerações relacionadas a isso para argumentar que as visões computacionais não são apenas falsas, mas carecem de um sentido claro. A computação, ou sintaxe, é “relativa ao observador”, não uma característica intrínseca da realidade: “…você pode atribuir uma interpretação computacional a qualquer coisa” (Searle 2002b, p. 17), até mesmo as moléculas da tinta na parede. Como nada é intrinsecamente computacional, não se pode ter uma teoria científica que reduza o mental, que não é relativo ao observador, à computação, que é. “A computação existe apenas em relação a algum agente ou observador que impõe uma interpretação computacional sobre algum fenômeno. Este é um ponto óbvio. Eu deveria tê-lo visto há dez anos, mas não o fiz. (Searle 2002b, p.17, publicado originalmente em 1993).

Os críticos observam que as paredes não são computadores; ao contrário de uma parede, um computador passa por transições de estado que são descritas contrafactualmente por um programa (Chalmers 1996, Block 2002, Haugeland 2002). Em seu artigo de 2002, Block aborda a questão de saber se uma parede é um computador (em resposta à acusação de Searle de que qualquer coisa mapeada em um sistema formal é um sistema formal, enquanto as mentes são bem diferentes). Block nega que se algo é ou não um computador depende inteiramente de nossa interpretação. Block observa que a Searle ignora os contrafactuais que devem ser verdadeiros de um sistema de implementação. Haugeland (2002) faz o mesmo argumento de que uma implementação será um processo causal que realiza as operações de forma confiável – e eles devem ser os poderes causais corretos. Block conclui que os argumentos de Searle falham,

Rey (2002) também aborda os argumentos de Searle de que a sintaxe e os símbolos são propriedades relativas ao observador, não físicas. Searle infere isso do fato de que as propriedades sintáticas (por exemplo, ser um “1” lógico) não são definidas na física; no entanto, Rey sustenta que não se segue que eles sejam relativos ao observador. Rey argumenta que Searle também não entende o que é realizar um programa. Rey endossa a resposta de Chalmers a Putnam: uma realização não é apenas um mapeamento estrutural, mas envolve causalidade, apoiando contrafactuais. “Esse ponto é esquecido com tanta frequência que vale a pena repetir: os objetos sintaticamente especificáveis ​​sobre os quais as computações são definidas podem e normalmente possuem uma semântica; é só que a semântica não está envolvida na especificação. ” Os estados de uma pessoa têm sua semântica em virtude da organização computacional e suas relações causais com o mundo. Rey conclui: Searle “simplesmente não considera os recursos substanciais do funcionalismo e da IA ​​forte”. (222) Uma história plausivelmente detalhada neutralizaria as conclusões negativas tiradas do esboço superficial do sistema na Sala Chinesa.

John Haugeland (2002) argumenta que há um sentido em que um processador deve compreender intrinsecamente os comandos dos programas que executa: ele os executa de acordo com as especificações. “A única maneira pela qual podemos entender um computador executando um programa é entendendo que seu processador responde às prescrições do programa como significativas” (385). Assim, os símbolos de operação têm significado para um sistema. Haugeland passa a fazer uma distinção entre sistema estreito e largo. Ele argumenta que os dados podem ter semântica no amplo sistema que inclui representações de objetos externos produzidos por transdutores. De passagem, Haugeland faz a afirmação incomum, defendida em outro lugar, de que a inteligência e a semântica genuínas pressupõem “a capacidade de um tipo de compromisso em como se vive” que não é proposicional – isto é, amor (cp.Inteligência Artificial: IA ).

À afirmação de Searle de que a sintaxe é relativa ao observador, que as moléculas em uma parede podem ser interpretadas como implementando o programa Wordstar (um programa de processamento de texto antigo) porque “existe algum padrão nos movimentos das moléculas que é isomórfico com a estrutura formal do Wordstar ” (Searle 1990b, p. 27), Haugeland responde que “a própria ideia de um símbolo sintático complexo... pressupõe processos específicos de escrita e leitura...” Os tokens devem ser sistematicamente produzidos e recuperáveis. Portanto, nenhum isomorfismo ou padrão aleatório em algum lugar (por exemplo, em alguma parede) vai contar e, portanto, a sintaxe não é relativa ao observador.

Com relação à questão de saber se alguém pode obter semântica da sintaxe, William Rapaport defendeu por muitos anos a “semântica sintática”, uma visão na qual o entendimento é uma forma especial de estrutura sintática na qual símbolos (como palavras chinesas) estão ligados a conceitos, eles mesmos representados sintaticamente. Outros acreditam que ainda não chegamos lá. IA futurista ( A Era das Máquinas Espirituais) Ray Kurzweil afirma em um livro de acompanhamento de 2002 que é uma pista falsa focar em computadores tradicionais de manipulação de símbolos. Kurzweil concorda com Searle que os computadores existentes não entendem a linguagem – como evidenciado pelo fato de que eles não conseguem se engajar em diálogos convincentes. Mas essa falha não afeta a capacidade dos futuros computadores baseados em tecnologia diferente. Kurzweil afirma que Searle falha em entender que as máquinas futuras usarão “métodos emergentes caóticos que são massivamente paralelos”. Essa afirmação parece ser semelhante à dos conexionistas, como Andy Clark, e à posição assumida pelos Churchlands em seu artigo de 1990 na Scientific American .

Além da alegação de Haugeland de que os processadores entendem as instruções do programa, os críticos de Searle podem concordar que os computadores não entendem a sintaxe mais do que entendem a semântica, embora, como todos os mecanismos causais, um computador tenha descrições sintáticas. E embora muitas vezes seja útil para os programadores tratar a máquina como se ela executasse operações sintáticas, nem sempre é assim: às vezes os caracteres que os programadores usam são apenas interruptores que fazem a máquina fazer algo, por exemplo, criar um determinado pixel no computador display fique vermelho ou faça a transmissão de um carro mudar de marcha. Assim, não está claro se Searle está correto quando diz que um computador digital é apenas “um dispositivo que manipula símbolos”. Os computadores são mecanismos causais complexos e as descrições sintáticas são úteis para estruturar as interconexões causais na máquina. Os programadores de IA enfrentam muitos problemas difíceis, mas pode-se afirmar que eles não precisam obter a semântica da sintaxe. Se quiserem obter semântica, devem obtê-la da causalidade.

Desenvolveram-se duas abordagens principais que explicam o significado em termos de conexões causais. As abordagens internalistas, como as abordagens de representação conceitual de Schank e Rapaport, e também a Semântica de Papel Conceitual, sustentam que um estado de um sistema físico obtém sua semântica de conexões causais com outros estados do mesmo sistema. Assim, um estado de um computador pode representar “kiwi” porque está conectado a nós “pássaros” e “sem voo”, e talvez também a imagens de kiwis prototípicos. O estado que representa a propriedade de ser “sem voo” pode obter seu conteúdo de um operador de negação modificando uma representação de “capaz de autopropulsão aérea”, e assim por diante, para formar uma vasta rede conceitual conectada, uma espécie de dicionário mental .

Externalista as abordagens desenvolvidas por Dennis Stampe, Fred Dretske, Hilary Putnam, Jerry Fodor, Ruth Millikan e outros sustentam que os estados de um sistema físico obtêm seu conteúdo por meio de conexões causais com a realidade externa que representam. Assim, grosso modo, um sistema com conceito KIWI é aquele que possui um estado que utiliza para representar a presença de kiwis no ambiente externo. Esse estado que representa o kiwi pode ser qualquer estado que esteja adequadamente conectado causalmente à presença de kiwis. Dependendo do sistema, o estado que representa o kiwi pode ser um estado de um cérebro, ou de um dispositivo elétrico como um computador, ou mesmo de um sistema hidráulico. O estado de representação interna pode, por sua vez, desempenhar um papel causal na determinação do comportamento do sistema. Por exemplo, Psicossemântica . Essas teorias semânticas que localizam conteúdo ou significado em relações causais apropriadas com o mundo se encaixam bem com a Resposta do Robô. Um computador em um corpo de robô pode ter apenas as conexões causais que permitem que seus estados sintáticos internos tenham a propriedade semântica de representar estados de coisas em seu ambiente.

Assim, há pelo menos duas famílias de teorias (e casamentos das duas, como em Block 1986) sobre como a semântica pode depender de conexões causais. Ambos tentam fornecer explicações que são neutras em termos de substância: estados de sistemas causais adequadamente organizados podem ter conteúdo, não importa do que os sistemas sejam feitos. De acordo com essas teorias, um computador poderia ter estados com significado. Não é necessário que o computador esteja ciente de seus próprios estados e saiba que eles têm significado, nem que alguém de fora aprecie o significado dos estados. Em qualquer uma dessas explicações, o significado depende das conexões causais (possivelmente complexas), e os computadores digitais são sistemas projetados para ter estados que possuem exatamente essas dependências causais complexas. Deve-se notar que Searle não concorda com essas teorias de semântica. Em vez de, intencionalidade .

5.2 Intencionalidade
A intencionalidade é a propriedade de ser sobre algo, ter conteúdo. No século XIX, o psicólogo Franz Brentano reintroduziu este termo da filosofia medieval e considerou que a intencionalidade era a “marca do mental”. Crenças e desejos são estados intencionais: eles têm conteúdo proposicional (acredita-se que p , deseja-se que p , onde sentenças que representam proposições substituem “ p”). As visões de Searle sobre a intencionalidade são complexas; O que é relevante aqui é que ele faz uma distinção entre a intencionalidade original ou intrínseca dos estados mentais genuínos e a intencionalidade derivada da linguagem. Uma frase escrita ou falada só tem intencionalidade derivada na medida em que é interpretada por alguém. Parece que, na visão de Searle, a intencionalidade original pode, pelo menos potencialmente, ser consciente. Searle então argumenta que a distinção entre intencionalidade original e derivada se aplica a computadores. Podemos interpretar os estados de um computador como tendo conteúdo, mas os próprios estados não têm intencionalidade original. Muitos filósofos endossam esse dualismo de intencionalidade, incluindo Sayre (1986) e até mesmo Fodor (2009), apesar das muitas diferenças de Fodor com Searle.

Em uma seção de seu livro de 1988, Computer Models of the Mind, Margaret Boden observa que a intencionalidade não é bem compreendida – razão para não colocar muito peso em argumentos que giram em torno da intencionalidade. Além disso, na medida em que entendemos o cérebro, nos concentramos nas funções informacionais, não nos poderes causais não especificados do cérebro: “... .” (241) Searle vê a intencionalidade como um poder causal do cérebro, exclusivamente produzido por processos biológicos. Dale Jacquette 1989 argumenta contra uma redução da intencionalidade – a intencionalidade, diz ele, é um “conceito primitivo ineliminável e irredutível”. No entanto, a maioria dos simpatizantes da IA ​​tem visto a intencionalidade, a cerca, como ligada à informação, e os estados não biológicos podem conter informações, assim como os estados cerebrais. Portanto, muitos respondentes a Searle argumentaram que ele exibe um chauvinismo de substância, ao sustentar que os cérebros entendem, mas os sistemas feitos de silício com capacidades comparáveis ​​de processamento de informações não podem, mesmo em princípio. Apareceram documentos de ambos os lados da questão, como o artigo de J. Maloney de 1987 “The Right Stuff”, defendendo Searle, e a crítica de R. Sharvy de 1983, “Não é a carne, é o movimento”. Os proponentes da IA, como Kurzweil (1999, ver também Richards 2002), continuaram a sustentar que os sistemas de IA podem potencialmente ter propriedades mentais como compreensão, inteligência, consciência e intencionalidade, e excederão as habilidades humanas nessas áreas. mesmo em princípio. Apareceram documentos de ambos os lados da questão, como o artigo de J. Maloney de 1987 “The Right Stuff”, defendendo Searle, e a crítica de R. Sharvy de 1983, “Não é a carne, é o movimento”. Os proponentes da IA, como Kurzweil (1999, ver também Richards 2002), continuaram a sustentar que os sistemas de IA podem potencialmente ter propriedades mentais como compreensão, inteligência, consciência e intencionalidade, e excederão as habilidades humanas nessas áreas. mesmo em princípio. Apareceram documentos de ambos os lados da questão, como o artigo de J. Maloney de 1987 “The Right Stuff”, defendendo Searle, e a crítica de R. Sharvy de 1983, “Não é a carne, é o movimento”. Os proponentes da IA, como Kurzweil (1999, ver também Richards 2002), continuaram a sustentar que os sistemas de IA podem potencialmente ter propriedades mentais como compreensão, inteligência, consciência e intencionalidade, e excederão as habilidades humanas nessas áreas.

Outros críticos da posição de Searle levam a intencionalidade mais a sério do que Boden, mas negam sua distinção dualista entre intencionalidade original e derivada. Dennett (1987, por exemplo) argumenta que toda intencionalidade é derivada, na medida em que as atribuições de intencionalidade – a animais, outras pessoas e até a nós mesmos – são instrumentais e nos permitem prever o comportamento, mas não são descrições de propriedades intrínsecas. Como vimos, Dennett está preocupado com a lentidão das coisas na Sala Chinesa, mas ele argumenta que, uma vez que um sistema está trabalhando em alta velocidade, ele tem tudo o que é necessário para a inteligência e a intencionalidade derivada – e a intencionalidade derivada é a única tipo que existe, de acordo com Dennett. Uma máquina pode ser um sistema intencional porque as explicações intencionais funcionam na previsão do comportamento da máquina. Dennett também sugere que Searle confunde intencionalidade com consciência da intencionalidade. Em seus argumentos sintático-semânticos, “Searle aparentemente confundiu uma afirmação sobre a inderivabilidade da semântica da sintaxe com uma afirmação sobre a inderivabilidade da consciência da semântica da sintaxe” (336). A ênfase na consciência nos obriga a pensar sobre as coisas do ponto de vista da primeira pessoa, mas Dennett 2017 continua pressionando a afirmação de que esse é um erro fundamental se quisermos entender o mental.

Também podemos nos preocupar que Searle confunda significado e interpretação, e que a intencionalidade original ou não derivada de Searle seja apenas uma intencionalidade de segunda ordem, uma representação do que um objeto intencional representa ou significa. Dretske e outros viram a intencionalidade como baseada em informações. Um estado do mundo, incluindo um estado em um computador, pode conter informações sobre outros estados no mundo, e essa condição informativa é uma característica dos estados independente da mente. Portanto, é um erro sustentar que as atribuições conscientes de significado são a fonte da intencionalidade.

Outros observaram que a discussão de Searle mostrou uma mudança ao longo do tempo de questões de intencionalidade e compreensão para questões de consciência. Searle liga a intencionalidade à consciência da intencionalidade, sustentando que os estados intencionais são pelo menos potencialmente conscientes. Em seu livro de 1996, The Conscious Mind, David Chalmers observa que, embora Searle originalmente direcione seu argumento contra a intencionalidade da máquina, fica claro a partir de escritos posteriores que a questão real é a consciência, que Searle sustenta ser uma condição necessária da intencionalidade. É a consciência que falta nos computadores digitais. Chalmers usa experimentos mentais para argumentar que é implausível que um sistema tenha alguma propriedade mental básica (como ter qualia) que outro sistema careça, se for possível imaginar a transformação de um sistema no outro, seja gradualmente (como a substituição de neurônios um em uma vez por circuitos digitais), ou todos de uma vez, alternando entre carne e silício.

Uma segunda estratégia em relação à atribuição de intencionalidade é adotada por críticos que, de fato, argumentam que a intencionalidade é uma característica intrínseca de estados de sistemas físicos que estão causalmente conectados com o mundo da maneira certa, independentemente da interpretação (consulte a seção anterior de Sintaxe e Semântica ). O externalismo semântico de Fodor é influenciado por Fred Dretske, mas eles chegam a conclusões diferentes com relação à semântica dos estados dos computadores. Ao longo dos anos, Dretske desenvolveu um relato histórico do significado ou conteúdo mental que impediria a atribuição de crenças e compreensão à maioria das máquinas. Dretske (1985) concorda com Searle que as máquinas de somar não somam literalmente; nósfazer a adição, usando as máquinas. Dretske enfatiza o papel crucial da seleção natural e do aprendizado na produção de estados que possuem conteúdo genuíno. Sistemas construídos por humanos serão, na melhor das hipóteses, como Swampmen (seres que resultam de um raio em um pântano e por acaso são uma cópia molécula por molécula de algum ser humano, digamos, você) – eles parecem ter intencionalidade ou estados, mas não o fazem, porque tais estados requerem o histórico correto. Os estados de IA geralmente serão falsificações de estados mentais reais; como dinheiro falso, eles podem parecer perfeitamente idênticos, mas carecem do pedigree correto. Mas a descrição da crença de Dretske parece torná-la distinta da percepção consciente da crença ou estado intencional (se isso for considerado como exigindo um pensamento de ordem superior),

Howard Gardiner endossa as críticas de Zenon Pylyshyn à visão de Searle sobre a relação entre cérebro e intencionalidade, como supondo que a intencionalidade é de alguma forma uma coisa “secretada pelo cérebro”, e o próprio experimento de contra-pensamento de Pylyshyn em que os neurônios de uma pessoa são substituídos um por um com workalikes de circuitos (veja também Cole e Foelber (1984) e Chalmers (1996) para exploração de cenários de substituição de neurônios). Gardiner sustenta que Searle nos deve uma descrição mais precisa da intencionalidade do que Searle deu até agora, e até então é uma questão em aberto se a IA pode produzi-la ou se está além de seu escopo. Gardiner conclui com a possibilidade de que a disputa entre Searle e seus críticos não seja científica, mas (quase?) religiosa.

5.3 Mente e Corpo
Vários críticos notaram que há questões metafísicas em jogo no argumento original. The Systems Reply chama a atenção para o problema metafísico da relação da mente com o corpo. Ele faz isso sustentando que a compreensão é uma propriedade do sistema como um todo, não do implementador físico. A Virtual Mind Reply afirma que mentes ou pessoas – as entidades que entendem e são conscientes – são mais abstratas do que qualquer sistema físico, e que pode haver uma relação de muitos para um entre mentes e sistemas físicos. (Mesmo que tudo seja físico, em princípio um único corpo poderia ser compartilhado por várias mentes, e uma única mente poderia ter uma sequência de corpos ao longo do tempo.) Assim, questões mais amplas sobre identidade pessoal e a relação entre mente e corpo estão em jogo em o debate entre Searle e alguns de seus críticos.

A visão de Searle é que o problema da relação entre mente e corpo “tem uma solução bastante simples. Aqui está: estados conscientes são causados ​​por processos neurobiológicos de nível inferior no cérebro e são eles próprios características de nível superior do cérebro” (Searle 2002b, p. 9). Em sua discussão inicial sobre o CRA, Searle falou sobre os poderes causais do cérebro. Assim, sua visão parece ser que os estados cerebrais causam consciência e compreensão, e “a consciência é apenas uma característica do cérebro” (ibid). No entanto, como vimos, mesmo que isso seja verdade, levanta-se a questão de saber de quem é a consciência que o cérebro cria. Os experimentos de cérebro dividido de Roger Sperry sugerem que talvez possa haver dois centros de consciência e, nesse sentido, duas mentes, implementadas por um único cérebro. Enquanto ambos exibem pelo menos alguma compreensão da linguagem, apenas um (normalmente criado pelo hemisfério esquerdo) controla a produção da linguagem. Assim, muitas abordagens atuais para entender a relação entre cérebro e consciência enfatizam a conexão e o fluxo de informações (ver, por exemplo, Dehaene 2014).

A consciência e a compreensão são características das pessoas, então parece que Searle aceita uma metafísica na qual eu, meu eu consciente, sou idêntico ao meu cérebro – uma forma de teoria da identidade mente-cérebro. Essa metafísica muito concreta é refletida na apresentação original de Searle do argumento CR, na qual a IA forte foi descrita por ele como a afirmação de que “o computador adequadamente programado é realmente uma mente” (Searle 1980). Esta é uma reivindicação de identidade e tem consequências estranhas. Se A e B são idênticos, qualquer propriedade de A é propriedade de B. Computadores são objetos físicos. Alguns computadores pesam 6 libras e possuem alto-falantes estéreo. Portanto, a alegação de que Searle chamou de IA forte implicaria que algumas mentes pesam 3 libras e têm alto-falantes estéreo. No entanto, parece claro que, embora os humanos possam pesar 150 libras; mentes humanas não pesam 150 libras. Isso sugere que nem os corpos nem as máquinas podem ser literalmente mentes. Tais considerações apóiam a visão de que as mentes são mais abstratas que os cérebros e, se assim for, pelo menos uma versão da afirmação que Searle chama de IA forte, a versão que diz que os computadores são literalmente mentes, é metafisicamente insustentável em face disso, exceto de qualquer experimento mental.

O argumento CR de Searle foi, portanto, dirigido contra a alegação de que um computador é uma mente, que um computador digital adequadamente programado entende a linguagem ou que seu programa o faz. O experimento mental de Searle apela para nossa forte intuição de que alguém que fizesse exatamente o que o computador faz não viria a entender chinês. Como observado acima, muitos críticos sustentam que Searle está certo neste ponto – não importa como você programe um computador, o computador não será literalmente uma mente e o computador não entenderá a linguagem natural. Mas se as mentes não são objetos físicos, essa incapacidade de um computador ser uma mente não mostra que a execução de um programa de IA não pode produzir compreensão da linguagem natural por algo diferente do computador (consulte a seção 4.1 acima).

Funcionalismo é uma teoria da relação das mentes com os corpos que foi desenvolvida nas duas décadas anteriores ao CRA de Searle. O funcionalismo é uma alternativa à teoria da identidade que está implícita em grande parte da discussão de Searle, bem como ao behaviorismo dominante em meados do século XX. Se o funcionalismo estiver correto, parece não haver nenhuma razão intrínseca para que um computador não possa ter estados mentais. Portanto, a conclusão do CRA de que um computador é intrinsecamente incapaz de estados mentais é uma consideração importante contra o funcionalismo. Julian Baggini (2009, 37) escreve que Searle “veio com talvez o contra-exemplo mais famoso da história – o argumento do quarto chinês – e em um soco intelectual infligiu tanto dano à então teoria dominante do funcionalismo que muitos argumentariam que nunca se recuperou.”

Os funcionalistas sustentam que um estado mental é o que um estado mental faz – o papel causal (ou “funcional”) que o estado desempenha determina qual estado ele é. Um funcionalista pode sustentar que a dor, por exemplo, é um estado tipicamente causado por danos ao corpo, está localizado em uma imagem corporal e é aversivo. Os funcionalistas se distanciam tanto dos behavioristas quanto dos teóricos da identidade. Em contraste com os primeiros, os funcionalistas sustentam que oprocessos causais são importantes para a posse de estados mentais. Assim, os funcionalistas podem concordar com Searle ao rejeitar o Teste de Turing como muito comportamental. Em contraste com os teóricos da identidade (que podem, por exemplo, sustentar que “a dor é idêntica ao disparo da fibra C”), os funcionalistas sustentam que os estados mentais podem ser obtidos por uma variedade de sistemas físicos (ou não físicos, como em Cole e Foelber 1984, em qual uma mente muda de uma implementação material para imaterial, neurônio por neurônio). Assim, enquanto um teórico da identidade identificará a dor com certos disparos de neurônios, um funcionalista identificará a dor com algo mais abstrato e de nível superior, um papel funcional que pode ser desempenhado por muitos tipos diferentes de sistemas subjacentes.

Os funcionalistas acusam os teóricos da identidade de chauvinismo substancial. No entanto, o funcionalismo permanece controverso: o funcionalismo é vulnerável às objeções do tipo nação chinesa discutidas acima, e os funcionalistas notoriamente têm problemas para explicar qualia, um problema destacado pela aparente possibilidade de um espectro invertido, onde estados qualitativamente diferentes podem ter o mesmo papel funcional (por exemplo, Block 1978, Maudlin 1989, Cole 1990).

computacionalismo é a subespécie do funcionalismo que sustenta que o importante papel causal dos processos cerebrais é o processamento de informações. Milkowski 2017 observa que as abordagens computacionais têm sido frutíferas na ciência cognitiva; ele examina as objeções ao computacionalismo e conclui que a maioria visa uma versão do espantalho. No entanto, Jerry Fodor, um dos primeiros proponentes de abordagens computacionais, argumenta em Fodor 2005 que os principais processos mentais, como a inferência para a melhor explicação, que dependem de propriedades não locais de representações, não podem ser explicados por módulos computacionais no cérebro. Se Fodor estiver certo, a compreensão da linguagem e da interpretação parece envolver considerações globais, como o contexto linguístico e não linguístico e a teoria da mente e, portanto, pode resistir à explicação computacional. Se for assim,

A declaração de 2010 da Searle sobre a conclusão do CRA mostra que as contas computacionais não podem explicar a consciência. Houve um interesse considerável nas décadas desde 1980 em determinar o que explica a consciência, e esta tem sido uma área de pesquisa extremamente ativa em todas as disciplinas. Um interesse tem sido nos correlatos neurais da consciência. Isso se relaciona diretamente com a afirmação de Searle de que a consciência é intrinsecamente biológica e não computacional ou processamento de informações. Ainda não há uma resposta definitiva, embora alguns trabalhos recentes sobre anestesia sugiram que a consciência é perdida quando as conexões corticais (e cortico-talâmicas) e o fluxo de informações são interrompidos (por exemplo, Hudetz 2012, um artigo de revisão).

Em geral, se for confirmado que a base da consciência está no nível relativamente abstrato do fluxo de informações através de redes neurais, ela será amigável ao funcionalismo, e se for inferior e mais biológica (ou subneuronal), será amigável para a conta da Searle.

Essas controversas questões biológicas e metafísicas sustentam a inferência central do argumento da Sala Chinesa. A partir da intuição de que no experimento de pensamento CR ele não entenderia chinês ao executar um programa, Searle deduz que não há entendimento criado ao executar um programa. Claramente, se essa inferência é válida ou não, gira em torno de uma questão metafísica sobre a identidade de pessoas e mentes. Se a pessoa que está entendendo não for idêntica ao operador da sala, a inferência não é sólida.

5.4 Simulação, duplicação e evolução
Ao discutir o CRA, Searle argumenta que há uma distinção importante entre simulação e duplicação. Ninguém confundiria uma simulação de computador do clima com clima, ou uma simulação de computador de digestão com digestão real. Searle conclui que é um erro igualmente grave confundir uma simulação de computador de compreensão com compreensão.

Em face disso, geralmente há uma distinção importante entre uma simulação e a coisa real. Mas surgem dois problemas. Não é claro que a distinção sempre possa ser feita. Os corações são biológicos, se alguma coisa é. Os corações artificiais são simulações de corações? Ou são duplicatas funcionais de corações, corações feitos de materiais diferentes? Andar é normalmente um fenômeno biológico realizado usando membros. Aqueles com membros artificiais andam? Ou simulam caminhar? Os robôs andam? Se as propriedades que são necessárias para ser certo tipo de coisa são propriedades de alto nível, qualquer coisa que compartilhe essas propriedades será uma coisa desse tipo, mesmo que difira em suas propriedades de nível inferior. Chalmers (1996) oferece um princípio que rege quando a simulação é replicação. Chalmers sugere que, contra Searle e Harnad (1989), uma simulação deX pode ser um X , nomeadamente quando a propriedade de ser um X é uma invariante organizacional, uma propriedade que depende apenas da organização funcional do sistema subjacente e não de quaisquer outros detalhes.

Copeland (2002) argumenta que a tese de Church-Turing não implica que o cérebro (ou toda máquina) pode ser simulado por uma máquina de Turing universal, pois o cérebro (ou outra máquina) pode ter operações primitivas que não são simples rotinas clericais que pode ser feito manualmente. (Um exemplo pode ser que os cérebros humanos provavelmente exibem aleatoriedade genuína de baixo nível, enquanto os computadores são cuidadosamente projetados para não fazer isso e, portanto, os computadores recorrem a números pseudoaleatórios quando a aleatoriedade aparente é necessária.) Sprevak 2007 levanta um ponto relacionado. A tese de Turing em 1938 em Princeton descrevia tais máquinas (“O-máquinas”). O-máquinas são máquinas que incluem funções de números naturais que não são computáveis ​​por máquina de Turing. Se o cérebro é uma dessas máquinas, então, diz Sprevak,: “Não há possibilidade de o argumento da Sala Chinesa de Searle ser implantado com sucesso contra a hipótese funcionalista de que o cérebro instancia uma O-machine…” (120).

Copeland discute a distinção simulação/duplicação em conexão com o Brain Simulator Reply. Ele argumenta que Searle observa corretamente que não se pode inferir de X simula Y , e Y tem a propriedade P , para a conclusão de que, portanto, X tem a propriedade P de Y para P arbitrário . Mas Copeland afirma que o próprio Searle comete a falácia da simulação ao estender o argumento CR da IA ​​tradicional para aplicar contra o computacionalismo. A contrapositiva da inferência é logicamente equivalente – X simula Y , X não tem Pportanto Y não – onde P entende chinês. O passo errado é: o operador CR S simula uma rede neural N , não é o caso que S entende chinês, portanto não é o caso que N entende chinês. Copeland também observa os resultados de Siegelmann e Sontag (1994) mostrando que algumas redes conexionistas não podem ser simuladas por uma Máquina de Turing universal (em particular, onde os pesos das conexões são números reais).

Há outro problema com a distinção simulação-duplicação, decorrente do processo de evolução. Searle deseja ver a intencionalidade original e a compreensão genuína como propriedades apenas de certos sistemas biológicos, presumivelmente o produto da evolução. Os computadores apenas simulam essas propriedades. Ao mesmo tempo, no cenário da Sala Chinesa, Searle sustenta que um sistema pode exibir um comportamento tão complexo quanto o humano, simulando qualquer grau de inteligência e compreensão de linguagem que se possa imaginar, e simulando qualquer habilidade de lidar com o mundo, mas não entender uma coisa. Ele também diz que tais sistemas comportamentais complexos podem ser implementados com materiais muito comuns, por exemplo, com tubos de água e válvulas.

Isso cria um problema biológico, além do problema das outras mentes observado pelos primeiros críticos do argumento do RC. enquanto nóspode pressupor que outros têm mentes, a evolução não faz tais pressuposições. As forças de seleção que impulsionam a evolução biológica selecionam com base no comportamento. A evolução pode selecionar a capacidade de usar informações sobre o ambiente de forma criativa e inteligente, desde que isso se manifeste no comportamento do organismo. Se não houver diferença evidente de comportamento em qualquer conjunto de circunstâncias entre um sistema que entende e outro que não entende, a evolução não pode selecionar o entendimento genuíno. E assim parece que, de acordo com Searle, as mentes que entendem genuinamente o significado não têm nenhuma vantagem sobre as criaturas que meramente processam informações, usando processos puramente computacionais. Assim, uma posição que implica que as simulações de compreensão podem ser tão biologicamente adaptáveis ​​quanto a coisa real, nos deixa com um enigma sobre como e por que sistemas com compreensão “genuína” poderiam evoluir. A intencionalidade original e a compreensão genuína tornam-se epifenomenais.

Conclusão
Como vimos, desde seu surgimento em 1980, o argumento da Sala Chinesa provocou discussões em várias disciplinas. Apesar da extensa discussão, ainda não há consenso sobre se o argumento é sólido. Em uma extremidade, temos a avaliação de Julian Baggini (2009) de que Searle “apresentou talvez o contra-exemplo mais famoso da história – o argumento do quarto chinês – e em um soco intelectual infligiu tantos danos à então teoria dominante do funcionalismo que muitos diria que nunca se recuperou.” Já o filósofo Daniel Dennett (2013, p. 320) conclui que o argumento da Sala Chinesa é “claramente um argumento falacioso e enganoso”. Portanto, não há consenso sobre se o argumento é uma prova que limita as aspirações da Inteligência Artificial ou as contas computacionais da mente.

Enquanto isso, o trabalho em inteligência artificial e processamento de linguagem natural continuou. O CRA levou Stevan Harnad e outros em uma busca pela “base simbólica” na IA. Muitos na filosofia (Dretske, Fodor, Millikan) trabalharam em teorias naturalistas do conteúdo mental. A especulação sobre a natureza da consciência continua em muitas disciplinas. E os computadores passaram do laboratório para o bolso e o pulso.

Na época da construção do argumento de Searle, os computadores pessoais eram dispositivos de hobby muito limitados. 'Eliza' de Weizenbaum e alguns jogos de 'aventura' de texto foram jogados em computadores DEC; estes incluíam analisadores limitados. A análise mais avançada da linguagem foi limitada a pesquisadores de computação, como Schank. Muita coisa mudou no próximo quarto de século; bilhões agora usam linguagem natural para interrogar e comandar agentes virtuais por meio de computadores que carregam em seus bolsos. O argumento da Sala Chinesa moderou as reivindicações daqueles que produzem IA e sistemas de linguagem natural? Alguns fabricantes que conectam dispositivos à “internet das coisas” fazem afirmações modestas: a fabricante de eletrodomésticos LG diz que a segunda década do século 21 traz a “experiência de conversar” com os principais eletrodomésticos. Isso pode ou não ser o mesmo que conversar. A Apple é menos cautelosa do que a LG ao descrever os recursos de seu aplicativo de “assistente pessoal virtual” chamado 'Siri': A Apple diz sobre a Siri que “ela entende o que você diz. Ele sabe o que você quer dizer. A IBM é rápida em afirmar que seu sistema 'Watson' muito maior é superior em habilidades de linguagem ao Siri. Em 2011, Watson venceu campeões humanos no game show de televisão 'Jeopardy', um feito que depende fortemente de habilidades de linguagem e inferência. A IBM continua afirmando que o que distingue o Watson é que ele “sabe o que sabe e sabe o que não sabe”. Isso parece reivindicar uma forma de autoconsciência ou consciência reflexiva para o sistema de computador Watson. Assim, as reivindicações de uma IA forte agora dificilmente são corrigidas e, se houver alguma coisa, algumas são mais fortes e exuberantes. Ao mesmo tempo, como vimos,

Embora separados por três séculos, Leibniz e Searle tiveram intuições semelhantes sobre os sistemas que consideram em seus respectivos experimentos mentais, Leibniz' Mill e Chinese Room. Em ambos os casos, eles consideram um sistema complexo composto de operações relativamente simples e observam que é impossível ver como poderia resultar a compreensão ou a consciência. Esses argumentos simples nos prestam o serviço de destacar os sérios problemas que enfrentamos na compreensão do significado e das mentes. As muitas questões levantadas pelo argumento da Sala Chinesa podem não ser resolvidas até que haja um consenso sobre a natureza do significado, sua relação com a sintaxe e sobre a base biológica da consciência. Continua a haver desacordo significativo sobre quais processos criam significado, compreensão e consciência,